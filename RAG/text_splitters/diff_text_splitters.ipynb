{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ebdc0af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90c5ce8",
   "metadata": {},
   "source": [
    "RecursiveCharacterTextSplitter\n",
    "\n",
    "CharacterTextSplitter\n",
    "\n",
    "TokenTextSplitter\n",
    "\n",
    "SemanticChunker\n",
    "\n",
    "NLTKTextSplitter\n",
    "\n",
    "SpacyTextSplitter\n",
    "\n",
    "MarkdownHeaderTextSplitter\n",
    "\n",
    "HTMLHeaderTextSplitter\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "45d9f9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_text_splitters.base import TokenTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_text_splitters.spacy import SpacyTextSplitter\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff48fe40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello world.\\n\\n', 'This is LangChain.\\n\\n', 'Splitter demo.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello world.\\n\\nThis is LangChain.\\n\\nSplitter demo.\"\n",
    "\n",
    "# keep_separator = True\n",
    "splitter_keep = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\"], \n",
    "    chunk_size=10,\n",
    "    chunk_overlap=0,\n",
    "    keep_separator='end' #start\n",
    ")\n",
    "print(splitter_keep.split_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72b2a919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello world.', '\\n\\nThis is LangChain.', '\\n\\nSplitter demo.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello world.\\n\\nThis is LangChain.\\n\\nSplitter demo.\"\n",
    "\n",
    "# keep_separator = True\n",
    "splitter_keep = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\"], \n",
    "    chunk_size=10,\n",
    "    chunk_overlap=0,\n",
    "    keep_separator=True\n",
    ")\n",
    "print(splitter_keep.split_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d99501ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello world.', 'This is LangChain.', 'Splitter demo.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "splitter_remove = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\"], \n",
    "    chunk_size=10,\n",
    "    chunk_overlap=0,\n",
    "    keep_separator=False\n",
    ")\n",
    "print(splitter_remove.split_text(text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f627bf90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Header', '---Body', 'cpr', '---Footer']\n"
     ]
    }
   ],
   "source": [
    "splitter_literal = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"---\",\"\\n\"], #we can see the sequenc of the splitter\n",
    "    is_separator_regex=False,\n",
    "    chunk_size=10,\n",
    "    chunk_overlap=0,\n",
    "    keep_separator=True\n",
    ")\n",
    "text = \"Header---Body\\ncpr---Footer\"\n",
    "print(splitter_literal.split_text(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad9d7c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Intro\\n paragraph.\\n\\nBody paragraph.', 'Conclusion.']\n"
     ]
    }
   ],
   "source": [
    "splitter_para = RecursiveCharacterTextSplitter(\n",
    "    separators=[r\"\\n{3,}\"],   # 3 or more consecutive newlines\n",
    "    is_separator_regex=True,\n",
    "    keep_separator=False,\n",
    "    chunk_size=10,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "text = \"Intro\\n paragraph.\\n\\nBody paragraph.\\n\\n\\nConclusion.\"\n",
    "chunks = splitter_para.split_text(text)\n",
    "print(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9881ede3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 66, which is longer than the specified 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '.\\n\\nMy name is Praveen.\\n\\nReddy.How are you. Hope you are doing good', 'How are you. Hope you are doing good.']\n"
     ]
    }
   ],
   "source": [
    "cts_default = CharacterTextSplitter(\n",
    "    chunk_size=10, \n",
    "    chunk_overlap=0,\n",
    "    separator=\",\",\n",
    ")\n",
    "text = \"Hello,.\\n\\nMy name is Praveen.\\n\\nReddy.How are you. Hope you are doing good,How are you. Hope you are doing good.\"\n",
    "print(cts_default.split_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba911a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Para1.\\n\\nPara2.\\n\\nPara3.']\n"
     ]
    }
   ],
   "source": [
    "cts_default = CharacterTextSplitter(\n",
    "    chunk_size=10, \n",
    "    chunk_overlap=0,\n",
    "    separator=\"\\n\",\n",
    ")\n",
    "text = \"Para1.\\n\\nPara2.\\n\\nPara3.\"\n",
    "print(cts_default.split_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32b7b3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 28, which is longer than the specified 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aarushi Reddy\\n\\nSahasra Reddy', 'Praveen Reddy']\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "cts_default = CharacterTextSplitter(\n",
    "    chunk_size=10, \n",
    "    chunk_overlap=0,\n",
    "    separator=r\"[.!?]\",\n",
    "    is_separator_regex=True\n",
    ")\n",
    "text = \"Aarushi Reddy\\n\\nSahasra Reddy.\\n\\nPraveen Reddy?\"\n",
    "print(cts_default.split_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624e142f",
   "metadata": {},
   "source": [
    "tiktoken is OpenAIâ€™s official tokenizer library.\n",
    "\n",
    "Here if u see even if the seperator is found, if the chunk_size is not satisfied then it is not split at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "173f0c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aarushi Reddy\\n\\nSahasra Reddy.\\n\\nPraveen Reddy?']\n"
     ]
    }
   ],
   "source": [
    "cts_tiktoken = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\",  \n",
    "    chunk_size=30, \n",
    "    chunk_overlap=0,\n",
    "    separator=\"\\n\\n\",\n",
    ")\n",
    "text = \"Aarushi Reddy\\n\\nSahasra Reddy.\\n\\nPraveen Reddy?\"\n",
    "print(cts_tiktoken.split_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2150ccd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aarushi Reddy?Sahasra Reddy.', 'Praveen Reddy Aarushi Reddy Sahasra Reddy.', 'Praveen Reddy Aarushi Reddy Sahasra Reddy.', 'Praveen Reddy Praveen Reddy Aarushi Reddy Sahasra Reddy.Praveen Reddy Aarushi Reddy Sahasra Reddy.']\n"
     ]
    }
   ],
   "source": [
    "cts_tiktoken = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\",  \n",
    "    chunk_size=20, \n",
    "    chunk_overlap=0,\n",
    "    separator=\"\\n\",\n",
    ")\n",
    "text = \"Aarushi Reddy?Sahasra Reddy.\\n\\nPraveen Reddy Aarushi Reddy Sahasra Reddy.\\n\\nPraveen Reddy Aarushi Reddy Sahasra Reddy.\\n\\nPraveen Reddy Praveen Reddy Aarushi Reddy Sahasra Reddy.Praveen Reddy Aarushi Reddy Sahasra Reddy.\"\n",
    "print(cts_tiktoken.split_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d0d1dab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aarushi Reddy?Sahasra Reddy.\\n\\nPraveen Reddy']\n"
     ]
    }
   ],
   "source": [
    "cts_tiktoken = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\",  \n",
    "    chunk_size=30, \n",
    "    chunk_overlap=0,\n",
    "    separator=\"?\",\n",
    ")\n",
    "text = \"Aarushi Reddy?Sahasra Reddy.\\n\\nPraveen Reddy?\"\n",
    "print(cts_tiktoken.split_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "58e0a07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My name is praveen reddy and i am running', ' running on the track so it is tracked My name is p', ' praveen reddy and i am running on the track', ' track so it is tracked My name is praveen redd', ' reddy and i am running on the track so it is', ' is tracked My name is praveen reddy and i', ' i am running on the track so it is tracked My name', ' name is praveen reddy and i am running on', ' on the track so it is tracked My name is prave', 'raveen reddy and i am running on the track so', ' so it is tracked My name is praveen reddy', 'y and i am running on the track so it is tracked', ' tracked My name is praveen reddy and i am', ' am running on the track so it is tracked My name is', ' is praveen reddy and i am running on the', ' the track so it is tracked My name is praveen', 'en reddy and i am running on the track so it', ' it is tracked ']\n"
     ]
    }
   ],
   "source": [
    "tts_default = TokenTextSplitter(\n",
    "    encoding_name=\"gpt2\", \n",
    "    chunk_size=12, \n",
    "    chunk_overlap=1, \n",
    ")\n",
    "text = \"My name is praveen reddy and i am running on the track so it is tracked \" * 10\n",
    "chunks = tts_default.split_text(text)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "40e732fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'set' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      1\u001b[39m tts_tiktoken = TokenTextSplitter.from_tiktoken_encoder(\n\u001b[32m      2\u001b[39m     encoding_name=\u001b[33m\"\u001b[39m\u001b[33mcl100k_base\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     model_name=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m     chunk_overlap=\u001b[32m30\u001b[39m,\n\u001b[32m      8\u001b[39m )\n\u001b[32m      9\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mMy name is > praveen reddy and < i am running on the track so it is tracked \u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m10\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m chunks = \u001b[43mtts_tiktoken\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Youtube_Repo/YouTube_Explain/RAG/.venv/lib/python3.11/site-packages/langchain_text_splitters/base.py:291\u001b[39m, in \u001b[36mTokenTextSplitter.split_text\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tokenizer.encode(\n\u001b[32m    279\u001b[39m         _text,\n\u001b[32m    280\u001b[39m         allowed_special=\u001b[38;5;28mself\u001b[39m._allowed_special,\n\u001b[32m    281\u001b[39m         disallowed_special=\u001b[38;5;28mself\u001b[39m._disallowed_special,\n\u001b[32m    282\u001b[39m     )\n\u001b[32m    284\u001b[39m tokenizer = Tokenizer(\n\u001b[32m    285\u001b[39m     chunk_overlap=\u001b[38;5;28mself\u001b[39m._chunk_overlap,\n\u001b[32m    286\u001b[39m     tokens_per_chunk=\u001b[38;5;28mself\u001b[39m._chunk_size,\n\u001b[32m    287\u001b[39m     decode=\u001b[38;5;28mself\u001b[39m._tokenizer.decode,\n\u001b[32m    288\u001b[39m     encode=_encode,\n\u001b[32m    289\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msplit_text_on_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Youtube_Repo/YouTube_Explain/RAG/.venv/lib/python3.11/site-packages/langchain_text_splitters/base.py:343\u001b[39m, in \u001b[36msplit_text_on_tokens\u001b[39m\u001b[34m(text, tokenizer)\u001b[39m\n\u001b[32m    341\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\u001b[39;00m\n\u001b[32m    342\u001b[39m splits: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] = []\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m input_ids = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    344\u001b[39m start_idx = \u001b[32m0\u001b[39m\n\u001b[32m    345\u001b[39m cur_idx = \u001b[38;5;28mmin\u001b[39m(start_idx + tokenizer.tokens_per_chunk, \u001b[38;5;28mlen\u001b[39m(input_ids))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Youtube_Repo/YouTube_Explain/RAG/.venv/lib/python3.11/site-packages/langchain_text_splitters/base.py:278\u001b[39m, in \u001b[36mTokenTextSplitter.split_text.<locals>._encode\u001b[39m\u001b[34m(_text)\u001b[39m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_encode\u001b[39m(_text: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallowed_special\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_allowed_special\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisallowed_special\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_disallowed_special\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Youtube_Repo/YouTube_Explain/RAG/.venv/lib/python3.11/site-packages/tiktoken/core.py:116\u001b[39m, in \u001b[36mEncoding.encode\u001b[39m\u001b[34m(self, text, allowed_special, disallowed_special)\u001b[39m\n\u001b[32m    114\u001b[39m     allowed_special = \u001b[38;5;28mself\u001b[39m.special_tokens_set\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m disallowed_special == \u001b[33m\"\u001b[39m\u001b[33mall\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     disallowed_special = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspecial_tokens_set\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mallowed_special\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m disallowed_special:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(disallowed_special, \u001b[38;5;28mfrozenset\u001b[39m):\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for -: 'set' and 'dict'"
     ]
    }
   ],
   "source": [
    "tts_tiktoken = TokenTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\",\n",
    "    model_name=None,\n",
    "    allowed_special={}, \n",
    "    disallowed_special=\"all\",\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=30,\n",
    ")\n",
    "text = \"My name is > praveen reddy and < i am running on the track so it is tracked \" * 10\n",
    "chunks = tts_tiktoken.split_text(text)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "27bfdeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My name is > praveen reddy and < i am running on the track so it is', ' tracked My name is > praveen reddy and < i am running on the track so it', ' is tracked My name is > praveen reddy and < i am running on the track so', ' it is tracked My name is > praveen reddy and < i am running on the track', ' so it is tracked My name is > praveen reddy and < i am running on the', ' track so it is tracked My name is > praveen reddy and < i am running on', ' the track so it is tracked My name is > praveen reddy and < i am running', ' on the track so it is tracked My name is > praveen reddy and < i am', ' running on the track so it is tracked My name is > praveen reddy and < i', ' am running on the track so it is tracked My name is > praveen reddy and <', ' i am running on the track so it is tracked ']\n"
     ]
    }
   ],
   "source": [
    "tts_tiktoken = TokenTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\",\n",
    "    model_name=None,\n",
    "    allowed_special=\"all\", \n",
    "    disallowed_special={\"-\"},\n",
    "    chunk_size=20,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "text = \"My name is > praveen reddy and < i am running on the track so it is tracked \" * 10\n",
    "chunks = tts_tiktoken.split_text(text)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3013f48e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Encountered text corresponding to disallowed special token '-'.\nIf you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'-', ...}`.\nIf you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'-'})`.\nTo disable this check for all special tokens, pass `disallowed_special=()`.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      1\u001b[39m tts_tiktoken = TokenTextSplitter.from_tiktoken_encoder(\n\u001b[32m      2\u001b[39m     encoding_name=\u001b[33m\"\u001b[39m\u001b[33mcl100k_base\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     model_name=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m     chunk_overlap=\u001b[32m0\u001b[39m,\n\u001b[32m      8\u001b[39m )\n\u001b[32m      9\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mMy name is > praveen reddy and < i am running - on the track so it is tracked \u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m10\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m chunks = \u001b[43mtts_tiktoken\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Youtube_Repo/YouTube_Explain/RAG/.venv/lib/python3.11/site-packages/langchain_text_splitters/base.py:291\u001b[39m, in \u001b[36mTokenTextSplitter.split_text\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tokenizer.encode(\n\u001b[32m    279\u001b[39m         _text,\n\u001b[32m    280\u001b[39m         allowed_special=\u001b[38;5;28mself\u001b[39m._allowed_special,\n\u001b[32m    281\u001b[39m         disallowed_special=\u001b[38;5;28mself\u001b[39m._disallowed_special,\n\u001b[32m    282\u001b[39m     )\n\u001b[32m    284\u001b[39m tokenizer = Tokenizer(\n\u001b[32m    285\u001b[39m     chunk_overlap=\u001b[38;5;28mself\u001b[39m._chunk_overlap,\n\u001b[32m    286\u001b[39m     tokens_per_chunk=\u001b[38;5;28mself\u001b[39m._chunk_size,\n\u001b[32m    287\u001b[39m     decode=\u001b[38;5;28mself\u001b[39m._tokenizer.decode,\n\u001b[32m    288\u001b[39m     encode=_encode,\n\u001b[32m    289\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msplit_text_on_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Youtube_Repo/YouTube_Explain/RAG/.venv/lib/python3.11/site-packages/langchain_text_splitters/base.py:343\u001b[39m, in \u001b[36msplit_text_on_tokens\u001b[39m\u001b[34m(text, tokenizer)\u001b[39m\n\u001b[32m    341\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\u001b[39;00m\n\u001b[32m    342\u001b[39m splits: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] = []\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m input_ids = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    344\u001b[39m start_idx = \u001b[32m0\u001b[39m\n\u001b[32m    345\u001b[39m cur_idx = \u001b[38;5;28mmin\u001b[39m(start_idx + tokenizer.tokens_per_chunk, \u001b[38;5;28mlen\u001b[39m(input_ids))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Youtube_Repo/YouTube_Explain/RAG/.venv/lib/python3.11/site-packages/langchain_text_splitters/base.py:278\u001b[39m, in \u001b[36mTokenTextSplitter.split_text.<locals>._encode\u001b[39m\u001b[34m(_text)\u001b[39m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_encode\u001b[39m(_text: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallowed_special\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_allowed_special\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisallowed_special\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_disallowed_special\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Youtube_Repo/YouTube_Explain/RAG/.venv/lib/python3.11/site-packages/tiktoken/core.py:121\u001b[39m, in \u001b[36mEncoding.encode\u001b[39m\u001b[34m(self, text, allowed_special, disallowed_special)\u001b[39m\n\u001b[32m    119\u001b[39m         disallowed_special = \u001b[38;5;28mfrozenset\u001b[39m(disallowed_special)\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m match := _special_token_regex(disallowed_special).search(text):\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m         \u001b[43mraise_disallowed_special_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._core_bpe.encode(text, allowed_special)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Youtube_Repo/YouTube_Explain/RAG/.venv/lib/python3.11/site-packages/tiktoken/core.py:439\u001b[39m, in \u001b[36mraise_disallowed_special_token\u001b[39m\u001b[34m(token)\u001b[39m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mraise_disallowed_special_token\u001b[39m(token: \u001b[38;5;28mstr\u001b[39m) -> NoReturn:\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    440\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEncountered text corresponding to disallowed special token \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    441\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIf you want this text to be encoded as a special token, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    442\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpass it to `allowed_special`, e.g. `allowed_special=\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m, ...\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    443\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIf you want this text to be encoded as normal text, disable the check for this token \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    444\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mby passing `disallowed_special=(enc.special_tokens_set - \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m)`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    445\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTo disable this check for all special tokens, pass `disallowed_special=()`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    446\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Encountered text corresponding to disallowed special token '-'.\nIf you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'-', ...}`.\nIf you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'-'})`.\nTo disable this check for all special tokens, pass `disallowed_special=()`.\n"
     ]
    }
   ],
   "source": [
    "tts_tiktoken = TokenTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\",\n",
    "    model_name=None,\n",
    "    allowed_special=\"all\", \n",
    "    disallowed_special={\"-\"},\n",
    "    chunk_size=20,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "text = \"My name is > praveen reddy and < i am running - on the track so it is tracked \" * 10\n",
    "chunks = tts_tiktoken.split_text(text)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "10c5eabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My name is > praveen reddy and < i am running on the track so it is', ' tracked My name is > praveen reddy and < i am running on the track so it', ' is tracked My name is > praveen reddy and < i am running on the track so', ' it is tracked My name is > praveen reddy and < i am running on the track', ' so it is tracked My name is > praveen reddy and < i am running on the', ' track so it is tracked My name is > praveen reddy and < i am running on', ' the track so it is tracked My name is > praveen reddy and < i am running', ' on the track so it is tracked My name is > praveen reddy and < i am', ' running on the track so it is tracked My name is > praveen reddy and < i', ' am running on the track so it is tracked My name is > praveen reddy and <', ' i am running on the track so it is tracked ']\n"
     ]
    }
   ],
   "source": [
    "tts_tiktoken = TokenTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\",\n",
    "    model_name=None,\n",
    "    allowed_special={\"<\", \"<\"}, \n",
    "    disallowed_special=\"all\",\n",
    "    chunk_size=20,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "text = \"My name is > praveen reddy and < i am running on the track so it is tracked \" * 10\n",
    "chunks = tts_tiktoken.split_text(text)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3247f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "hf_tok = AutoTokenizer.from_pretrained(\"gpt2\") \n",
    "tts_hf = TokenTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer=hf_tok,\n",
    "    chunk_size=60,  \n",
    "    chunk_overlap=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7602e84e",
   "metadata": {},
   "source": [
    "For threshold_type=percentile, if the breakpoint_threshold_amount value is lower then, more splits happen and even for small change in the meaning it will split the data, as the number in higher then rarely split happens \n",
    "\n",
    "80 percentile : Find the similarity score of the embeded vectors thatâ€™s lower than 80% of all others â€” and split wherever the similarity is below that threshold.\n",
    "If you are at the 90th percentile, you scored better than 90 out of 100 students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "03dea303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='\\nThere is a movie called Once upon a time in hollywood. I love the movie and the songs are also good.'),\n",
       " Document(metadata={}, page_content='Its a must watch movie. LangChain is a powerful framework for building LLM applications. It helps developers connect data sources, tools, and models.'),\n",
       " Document(metadata={}, page_content='Many use it to build chatbots and RAG systems. Pandas is a Python library for data analysis. It provides DataFrame structures, filtering, and grouping utilities. Data scientists use Pandas for preprocessing and analytics. The capital of France is Paris.'),\n",
       " Document(metadata={}, page_content='It is famous for the Eiffel Tower and French cuisine. Paris is a global center for fashion and art. The human heart pumps blood through the body. It maintains circulation and oxygen supply. ')]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = SemanticChunker(\n",
    "    embeddings = OpenAIEmbeddings(),\n",
    "    buffer_size= 1, #\n",
    "    breakpoint_threshold_type = \"percentile\", #Literal[\"percentile\", \"standard_deviation\"]\n",
    "    breakpoint_threshold_amount = 80,\n",
    "    )\n",
    "content = content = \"\"\"\n",
    "There is a movie called Once upon a time in hollywood. I love the movie and the songs are also good. Its a must watch movie.\n",
    "LangChain is a powerful framework for building LLM applications.\n",
    "It helps developers connect data sources, tools, and models.\n",
    "Many use it to build chatbots and RAG systems.\n",
    "Pandas is a Python library for data analysis.\n",
    "It provides DataFrame structures, filtering, and grouping utilities.\n",
    "Data scientists use Pandas for preprocessing and analytics.\n",
    "The capital of France is Paris.\n",
    "It is famous for the Eiffel Tower and French cuisine.\n",
    "Paris is a global center for fashion and art.\n",
    "The human heart pumps blood through the body.\n",
    "It maintains circulation and oxygen supply.\n",
    "\"\"\"\n",
    "docs = text_splitter.create_documents([content])\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7625633f",
   "metadata": {},
   "source": [
    "standard_deviation compares each similarity to the average â€” if itâ€™s unusually low (by more than X standard deviations), thatâ€™s a new topic â€” split here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "29501eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='\\nThere is a movie called Once upon a time in hollywood. I love the movie and the songs are also good. Its a must watch movie. LangChain is a powerful framework for building LLM applications. It helps developers connect data sources, tools, and models.'),\n",
       " Document(metadata={}, page_content='Many use it to build chatbots and RAG systems. Pandas is a Python library for data analysis. It provides DataFrame structures, filtering, and grouping utilities. Data scientists use Pandas for preprocessing and analytics. The capital of France is Paris. It is famous for the Eiffel Tower and French cuisine. Paris is a global center for fashion and art. The human heart pumps blood through the body. It maintains circulation and oxygen supply. ')]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = SemanticChunker(\n",
    "    embeddings = OpenAIEmbeddings(),\n",
    "    buffer_size= 1, #\n",
    "    breakpoint_threshold_type = \"standard_deviation\", #Literal[\"percentile\", \"standard_deviation\"]\n",
    "    breakpoint_threshold_amount = 2.0,\n",
    "    )\n",
    "content = content = \"\"\"\n",
    "There is a movie called Once upon a time in hollywood. I love the movie and the songs are also good. Its a must watch movie.\n",
    "LangChain is a powerful framework for building LLM applications.\n",
    "It helps developers connect data sources, tools, and models.\n",
    "Many use it to build chatbots and RAG systems.\n",
    "Pandas is a Python library for data analysis.\n",
    "It provides DataFrame structures, filtering, and grouping utilities.\n",
    "Data scientists use Pandas for preprocessing and analytics.\n",
    "The capital of France is Paris.\n",
    "It is famous for the Eiffel Tower and French cuisine.\n",
    "Paris is a global center for fashion and art.\n",
    "The human heart pumps blood through the body.\n",
    "It maintains circulation and oxygen supply.\n",
    "\"\"\"\n",
    "docs = text_splitter.create_documents([content])\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f55cc315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='\\nThere is a movie called Once upon a time in hollywood. I love the movie and the songs are also good.'),\n",
       " Document(metadata={}, page_content='Its a must watch movie. LangChain is a powerful framework for building LLM applications. It helps developers connect data sources, tools, and models.'),\n",
       " Document(metadata={}, page_content='Many use it to build chatbots and RAG systems. Pandas is a Python library for data analysis. It provides DataFrame structures, filtering, and grouping utilities. Data scientists use Pandas for preprocessing and analytics. The capital of France is Paris. It is famous for the Eiffel Tower and French cuisine. Paris is a global center for fashion and art. The human heart pumps blood through the body. It maintains circulation and oxygen supply. ')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = SemanticChunker(\n",
    "    embeddings = OpenAIEmbeddings(),\n",
    "    buffer_size= 1, #\n",
    "    breakpoint_threshold_type = \"standard_deviation\", #Literal[\"percentile\", \"standard_deviation\"]\n",
    "    breakpoint_threshold_amount = 1.0,\n",
    "    )\n",
    "content = content = \"\"\"\n",
    "There is a movie called Once upon a time in hollywood. I love the movie and the songs are also good. Its a must watch movie.\n",
    "LangChain is a powerful framework for building LLM applications.\n",
    "It helps developers connect data sources, tools, and models.\n",
    "Many use it to build chatbots and RAG systems.\n",
    "Pandas is a Python library for data analysis.\n",
    "It provides DataFrame structures, filtering, and grouping utilities.\n",
    "Data scientists use Pandas for preprocessing and analytics.\n",
    "The capital of France is Paris.\n",
    "It is famous for the Eiffel Tower and French cuisine.\n",
    "Paris is a global center for fashion and art.\n",
    "The human heart pumps blood through the body.\n",
    "It maintains circulation and oxygen supply.\n",
    "\"\"\"\n",
    "docs = text_splitter.create_documents([content])\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1c79443f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='\\nThere is a movie called Once upon a time in hollywood. I love the movie and the songs are also good.'),\n",
       " Document(metadata={}, page_content='Its a must watch movie. LangChain is a powerful framework for building LLM applications. It helps developers connect data sources, tools, and models.'),\n",
       " Document(metadata={}, page_content='Many use it to build chatbots and RAG systems. Pandas is a Python library for data analysis. It provides DataFrame structures, filtering, and grouping utilities.'),\n",
       " Document(metadata={}, page_content='Data scientists use Pandas for preprocessing and analytics. The capital of France is Paris.'),\n",
       " Document(metadata={}, page_content='It is famous for the Eiffel Tower and French cuisine. Paris is a global center for fashion and art. The human heart pumps blood through the body.'),\n",
       " Document(metadata={}, page_content='It maintains circulation and oxygen supply. ')]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = SemanticChunker(\n",
    "    embeddings = OpenAIEmbeddings(),\n",
    "    buffer_size= 1, #\n",
    "    breakpoint_threshold_type = \"standard_deviation\", #Literal[\"percentile\", \"standard_deviation\"]\n",
    "    breakpoint_threshold_amount = 0.3,\n",
    "    )\n",
    "content = content = \"\"\"\n",
    "There is a movie called Once upon a time in hollywood. I love the movie and the songs are also good. Its a must watch movie.\n",
    "LangChain is a powerful framework for building LLM applications.\n",
    "It helps developers connect data sources, tools, and models.\n",
    "Many use it to build chatbots and RAG systems.\n",
    "Pandas is a Python library for data analysis.\n",
    "It provides DataFrame structures, filtering, and grouping utilities.\n",
    "Data scientists use Pandas for preprocessing and analytics.\n",
    "The capital of France is Paris.\n",
    "It is famous for the Eiffel Tower and French cuisine.\n",
    "Paris is a global center for fashion and art.\n",
    "The human heart pumps blood through the body.\n",
    "It maintains circulation and oxygen supply.\n",
    "\"\"\"\n",
    "docs = text_splitter.create_documents([content])\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3e6452",
   "metadata": {},
   "source": [
    "python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef2a441",
   "metadata": {},
   "source": [
    "SpacyTextSplitter uses grammar rules, punctuation, and context to split the text \n",
    "\n",
    "We have NLTKTextSplitter but it won't understand the grammar rules, punctuation as much as spacy does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b307fbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 245, which is longer than the specified 10\n",
      "Created a chunk of size 34, which is longer than the specified 10\n",
      "Created a chunk of size 53, which is longer than the specified 10\n",
      "Created a chunk of size 39, which is longer than the specified 10\n",
      "Created a chunk of size 28, which is longer than the specified 10\n",
      "Created a chunk of size 31, which is longer than the specified 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: ['LangChain is a powerful framework It lets you build retrieval-augmented systems LangChain is a powerful framework It lets you build retrieval-augmented systems LangChain is a powerful framework It lets you build retrieval-augmented systems.', 'LangChain is a powerful framework.', 'It lets you build retrieval-augmented systems.', 'SpaCy helps with sentence segmentation.', 'It knows linguistics.', 'Paris is the capital of France.', 'Itâ€™s known for the Eiffel Tower.']\n"
     ]
    }
   ],
   "source": [
    "content = \"\"\"LangChain is a powerful framework It lets you build retrieval-augmented systems LangChain is a powerful framework It lets you build retrieval-augmented systems LangChain is a powerful framework It lets you build retrieval-augmented systems.\n",
    "    LangChain is a powerful framework. It lets you build retrieval-augmented systems.\\n\\n\n",
    "    SpaCy helps with sentence segmentation. It knows linguistics.\\n\\n\n",
    "    Paris is the capital of France. Itâ€™s known for the Eiffel Tower.\n",
    "\"\"\"\n",
    "splitter = SpacyTextSplitter(separator=\"\\n\\n\", pipeline=\"en_core_web_sm\", strip_whitespace=True,chunk_size=10,chunk_overlap=0)\n",
    "chunks = splitter.split_text(content)\n",
    "print(\"Chunks:\", chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38995282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004e2a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF: AIF_Guidelines_English_12Jun24.pdf\n",
      "Loaded 20 raw document(s). Splitting into chunks...\n",
      "Using chunk size: 300, overlap: 150\n",
      "\n",
      "\n",
      " This is the chunk --> 1 \n",
      " \n",
      " \n",
      "Scheme Guidelines \n",
      "for \n",
      "CENTRAL SECTOR SCHEME \n",
      "of \n",
      "Financing facility under â€˜Agriculture Infrastructure Fundâ€™ \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Revised Scheme Guidelines \n",
      " \n",
      "January 2023 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Department of Agriculture & Farmers Welfare \n",
      "Ministry of Agriculture & Farmers Welfare \n",
      "Governmen <-- \n",
      "\n",
      "\n",
      "Processing page 1 with length 310 characters\n",
      "Doc 0: split into 1 chunks\n",
      "chunk size: 1\n",
      "\n",
      "\n",
      " This is the chunk --> 2 \n",
      "Index \n",
      " \n",
      " \n",
      "S.\n",
      "No\n",
      ". \n",
      "Particulars Page No. \n",
      "1 Introduction 3 \n",
      "2 Rationale of the Scheme 3-4 \n",
      "3 Objectives of Scheme 4-5 \n",
      "4 Implementation Period of Scheme 5 \n",
      "5. Government Budgetary Support 6 \n",
      "6. Eligible Projects 7-10 \n",
      "7. Size of the financing facility& eligible beneficiaries 10 \n",
      "8. Number of Proj <-- \n",
      "\n",
      "\n",
      "Processing page 2 with length 765 characters\n",
      "Doc 1: split into 1 chunks\n",
      "chunk size: 1\n",
      "\n",
      "\n",
      " This is the chunk --> 3 \n",
      "Scheme Guidelines for CENTRAL SECTOR SCHEME of financing facility \n",
      "under â€˜Agriculture Infrastructure Fundâ€™ \n",
      " \n",
      "1 Introduction \n",
      " \n",
      "The role of infrastructure is crucial for agriculture development and for taking the \n",
      "production dynamics to the next level. It is only through the development of infras <-- \n",
      "\n",
      "\n",
      "Processing page 3 with length 2032 characters\n",
      "Doc 2: split into 1 chunks\n",
      "chunk size: 1\n",
      "\n",
      "\n",
      " This is the chunk --> 4 \n",
      "majority of the farmers is very low. Further, India has limited infrastructure connecting \n",
      "farmers to markets and hence, 15 -20% of yield is wasted which is relatively high in \n",
      "comparison to other countries where it ranges between 5 -15%. Investment in agriculture in \n",
      "India has further been stagn <-- \n",
      "\n",
      "\n",
      "Processing page 4 with length 2315 characters\n",
      "Doc 3: split into 1 chunks\n",
      "chunk size: 1\n",
      "\n",
      "\n",
      " This is the chunk --> 5 \n",
      "convergence and credit guarantee. This will initiate the cycle of innovation and \n",
      "private sector investment in agriculture. \n",
      "- Due to improvements in post -harvest infrastructure, government will further be \n",
      "able to reduce national food wastage percentage thereby enable agriculture \n",
      "sector to bec <-- \n",
      "\n",
      "\n",
      "Processing page 5 with length 2066 characters\n",
      "Doc 4: split into 1 chunks\n",
      "chunk size: 1\n",
      "\n",
      "\n",
      " This is the chunk --> 6 \n",
      "moratorium period of up to 2 years. \n",
      " \n",
      "5 Government Budgetary Support \n",
      " \n",
      "Budgetary support will be provided for interest subvention and credit guarantee fee as \n",
      "also administrative cost of PMU. The details are as below:- \n",
      " \n",
      "Sl.No. Name of \n",
      "Component \n",
      "Norms \n",
      "1 Interest \n",
      "Subvention Cost \n",
      "All loans  <-- \n",
      "\n",
      "\n",
      "Processing page 6 with length 1528 characters\n",
      "Doc 5: split into 1 chunks\n",
      "chunk size: 1\n",
      "\n",
      "\n",
      " This is the chunk --> 7 \n",
      "6 Eligible Projects \n",
      "The scheme will facilitate setting up and modernization of key elements of the value \n",
      "chain including \n",
      "Following projects are eligible for all beneficiaries including private entities as well as groups \n",
      "such as FPOs, PACS, SHGs, JLGs, Cooperatives, National and State Level Fe <-- \n",
      "\n",
      "\n",
      "Processing page 7 with length 1976 characters\n",
      "Doc 6: split into 1 chunks\n",
      "chunk size: 1\n",
      "\n",
      "\n",
      " This is the chunk --> 8 \n",
      "Following projects are eligible for only groups such as FPOs, PACS, SHGs, JLGs, \n",
      "Cooperatives, National and State Level Federation of Co -operatives, FPOs federations, \n",
      "Federations of SHGs, National and State Level Agencies as they qualify as community \n",
      "farming assets \n",
      " \n",
      " \n",
      "ï‚§ Hydroponic Farming \n",
      "ï‚§ <-- \n",
      "\n",
      "\n",
      "Processing page 8 with length 1999 characters\n",
      "Doc 7: split into 1 chunks\n",
      "chunk size: 1\n",
      "\n",
      "\n",
      " This is the chunk --> 9 \n",
      "Sl. No. Crops Eligible Activities Ineligible \n",
      "Activities \n",
      "Bengal Gram , Pigeon Peas, \n",
      "Green Gram, Chick Peas, Black \n",
      "Gram, Red Kidney Beans, Black \n",
      "Eyed Peas, White Peas Etc. \n",
      "& Grading, De-Husking, Splitting, De-\n",
      "Hulling, Milling (Besan) , Irradiation, \n",
      "Packaging, Storage \n",
      "like Papads, Pulse \n",
      "ba <-- \n",
      "\n",
      "\n",
      "Processing page 9 with length 2235 characters\n",
      "Doc 8: split into 1 chunks\n",
      "chunk size: 1\n",
      "\n",
      "\n",
      " This is the chunk --> 10 \n",
      "Sl. No. Crops Eligible Activities Ineligible \n",
      "Activities \n",
      "Barberry, Liquorice, Bael, \n",
      "Isabgol,Guggal, Kerth, Aonla, \n",
      "Chandan, Senna, Baiberang, \n",
      "Brahmi, Eucalyptus, Jatamansi \n",
      "Etc. \n",
      "9 Bamboo Drying, Cutting, Stripping, Formation \n",
      "Of Sheets, Bamboo Charcoal , \n",
      "Powder, Granules, Bamboo \n",
      "Treatment  <-- \n",
      "\n",
      "\n",
      "Processing page 10 with length 1757 characters\n",
      "Doc 9: split into 1 chunks\n",
      "chunk size: 1\n",
      "\n",
      "\n",
      " This is the chunk --> 11 \n",
      "8 Number of Projects per entity eligible under the scheme \n",
      " \n",
      "Interest subvention for a loan upto â‚¹2 crore in one location is eligible under the \n",
      "scheme. Multiple projects in one location are also eligible with an overall cap of â‚¹2 crore. ln \n",
      "case, one eligible entity puts up projects in differen <-- \n",
      "\n",
      "\n",
      "Processing page 11 with length 2287 characters\n",
      "Doc 10: split into 1 chunks\n",
      "chunk size: 1\n",
      "\n",
      "\n",
      " This is the chunk --> 12 \n",
      "negotiate cap on lending rates in a fair manner. \n",
      " \n",
      "12 Project Management and handholding support \n",
      "An online platform will be made available in collaboration with participating lending \n",
      "institutions to provide information and loan sanctioning facility. Agri Infra fund will be \n",
      "managed and monito <-- \n",
      "\n",
      "\n",
      "Processing page 12 with length 2132 characters\n",
      "Doc 11: split into 1 chunks\n",
      "chunk size: 1\n",
      "\n",
      "\n",
      " This is the chunk --> 13 \n",
      "Concurrent/mid-term third party independent evaluation of the scheme in addition to end - line \n",
      "evaluation will be conducted as and when required. \n",
      " \n",
      " \n",
      "15 Monitoring framework \n",
      "The National, State and District L evel Monitoring Committees to ensure real -time \n",
      "monitoring and effective feed -back <-- \n",
      "\n",
      "\n",
      "Processing page 13 with length 1565 characters\n",
      "Doc 12: split into 1 chunks\n",
      "chunk size: 1\n",
      "\n",
      "\n",
      " This is the chunk --> 14 \n",
      " \n",
      "20 Sector specific focus \n",
      "24% of total grants â€“ in â€“ aid under the scheme should be utilized for SC/ST \n",
      "entrepreneurs (16% for SC and 8% for ST). Besides this, lending institutions would ensure \n",
      "adequate coverage of entrepreneurs belonging to women and other weaker segments of \n",
      "society may be  <-- \n",
      "\n",
      "\n",
      "Processing page 14 with length 716 characters\n",
      "Doc 13: split into 1 chunks\n",
      "chunk size: 1\n",
      "\n",
      "\n",
      " This is the chunk --> 15 \n",
      "Annexure-A \n",
      " \n",
      "Monitoring framework \n",
      " \n",
      " \n",
      "(i) National level Monitoring Committee (NLMC) \n",
      " \n",
      "Composition :- \n",
      " \n",
      "Following will be the Members and Chairman of the NLMC:- \n",
      " \n",
      "a. Secretary (DA&FW) (Chairman) \n",
      "b. MD SFAC \n",
      "c. MD,NCDC \n",
      "d. Special Secretary/Additional Secretary and FA (DA&FW) \n",
      "e. Additional <-- \n",
      "\n",
      "\n",
      "Processing page 15 with length 1567 characters\n",
      "Doc 14: split into 1 chunks\n",
      "chunk size: 1\n",
      "\n",
      "\n",
      " This is the chunk --> 16 \n",
      "j. Functions:- \n",
      " \n",
      "1. State level Monitoring Committee (SLMC) will implement the NIMC guidelines at the state \n",
      "level and provide feedback to NIMC. \n",
      "2. It will also guide and steer the implementation of the scheme in the state. \n",
      "3. It will set the targets as per OOMF format and review the progress <-- \n",
      "\n",
      "\n",
      "Processing page 16 with length 1384 characters\n",
      "Doc 15: split into 1 chunks\n",
      "chunk size: 1\n",
      "\n",
      "\n",
      " This is the chunk --> 17 \n",
      "Annexure-B \n",
      "OOMF Framework \n",
      " \n",
      "OUTPUTS: \n",
      " \n",
      "Output statement Output Indicators Target Achievement \n",
      "Promoting creation \n",
      "and modernization of \n",
      "agriculture \n",
      "infrastructure \n",
      "No. of projects submitted by \n",
      "eligible entities \n",
      " \n",
      "Disbursement of funds for \n",
      "eligible projects/investments \n",
      "(â‚¹Crore) \n",
      " \n",
      "Increas <-- \n",
      "\n",
      "\n",
      "Processing page 17 with length 743 characters\n",
      "Doc 16: split into 1 chunks\n",
      "chunk size: 1\n",
      "\n",
      "\n",
      " This is the chunk --> 18 \n",
      "Outcome Statement Outcome Indicators Target Achievement \n",
      "Improvement in \n",
      "resource provision for \n",
      "agriculture \n",
      "infrastructure \n",
      "Percentage of fund utilized \n",
      "for completed projects \n",
      " \n",
      "Additional investments \n",
      "leveraged due to \n",
      "agriculture infrastructure \n",
      "fund interventions (â‚¹ \n",
      "Crore) \n",
      " \n",
      "Enhancement  <-- \n",
      "\n",
      "\n",
      "Processing page 18 with length 497 characters\n",
      "Doc 17: split into 1 chunks\n",
      "chunk size: 1\n",
      "\n",
      "\n",
      " This is the chunk --> 19 \n",
      "Annexure-C \n",
      "Tentative Allocation of Financing Facility among States/UTs \n",
      " \n",
      "Sl. \n",
      "No. \n",
      "State Financing Facility INR Crore \n",
      "1 Uttar Pradesh 12831 \n",
      "2 Rajasthan 9015 \n",
      "3 Maharashtra 8460 \n",
      "4 Madhya Pradesh 7440 \n",
      "5 Gujarat 7282 \n",
      "6 West Bengal 7260 \n",
      "7 Andhra Pradesh 6540 \n",
      "8 Tamil Nadu 5990 \n",
      "9 Punjab 4713 <-- \n",
      "\n",
      "\n",
      "Processing page 19 with length 578 characters\n",
      "Doc 18: split into 1 chunks\n",
      "chunk size: 1\n",
      "\n",
      "\n",
      " This is the chunk --> 20 \n",
      "Sl. \n",
      "No. \n",
      "State Financing Facility INR Crore \n",
      "24 Nagaland 230 \n",
      "25 Manipur 200 \n",
      "26 Mizoram 196 \n",
      "27 Meghalaya 190 \n",
      "28 Goa 110 \n",
      "29 Delhi 102 \n",
      "30 Sikkim 56 \n",
      "31 Puducherry 48 \n",
      "32 A & N Islands 40 \n",
      "33 Daman & Diu 22 \n",
      "34 Lakshadweep 11 \n",
      "35 Dadra & Nagar Haveli 10 \n",
      "36 Chandigarh 9 \n",
      " Total 1,00,000 <-- \n",
      "\n",
      "\n",
      "Processing page 20 with length 294 characters\n",
      "Doc 19: split into 1 chunks\n",
      "chunk size: 1\n",
      "Total chunks created: 20\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"AIF_Guidelines_English_12Jun24.pdf\"\n",
    "docs = load_and_chunk_pdf(str(pdf_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c4d9ec14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 245, which is longer than the specified 10\n",
      "Created a chunk of size 34, which is longer than the specified 10\n",
      "Created a chunk of size 53, which is longer than the specified 10\n",
      "Created a chunk of size 39, which is longer than the specified 10\n",
      "Created a chunk of size 28, which is longer than the specified 10\n",
      "Created a chunk of size 31, which is longer than the specified 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: ['LangChain is a powerful framework It lets you build retrieval-augmented systems LangChain is a powerful framework It lets you build retrieval-augmented systems LangChain is a powerful framework It lets you build retrieval-augmented systems.', 'LangChain is a powerful framework.', 'It lets you build retrieval-augmented systems.', 'SpaCy helps with sentence segmentation.', 'It knows linguistics.', 'Paris is the capital of France.', 'Itâ€™s known for the Eiffel Tower.']\n"
     ]
    }
   ],
   "source": [
    "content = \"\"\"LangChain is a powerful framework It lets you build retrieval-augmented systems LangChain is a powerful framework It lets you build retrieval-augmented systems LangChain is a powerful framework It lets you build retrieval-augmented systems.\n",
    "    LangChain is a powerful framework. It lets you build retrieval-augmented systems.\\n\\n\n",
    "    SpaCy helps with sentence segmentation. It knows linguistics.\\n\\n\n",
    "    Paris is the capital of France. Itâ€™s known for the Eiffel Tower.\n",
    "\"\"\"\n",
    "splitter = SpacyTextSplitter(separator=\"\\n\\n\", pipeline=\"en_core_web_sm\", strip_whitespace=True,chunk_size=10,chunk_overlap=0)\n",
    "chunks = splitter.split_text(content)\n",
    "print(\"Chunks:\", chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8cce47e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 245, which is longer than the specified 40\n",
      "Created a chunk of size 53, which is longer than the specified 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: ['LangChain is a powerful framework It lets you build retrieval-augmented systems LangChain is a powerful framework It lets you build retrieval-augmented systems LangChain is a powerful framework It lets you build retrieval-augmented systems.', 'LangChain is a powerful framework.', 'It lets you build retrieval-augmented systems.', 'SpaCy helps with sentence segmentation.', 'It knows linguistics.', 'Paris is the capital of France.', 'Itâ€™s known for the Eiffel Tower.']\n"
     ]
    }
   ],
   "source": [
    "content = \"\"\"LangChain is a powerful framework It lets you build retrieval-augmented systems LangChain is a powerful framework It lets you build retrieval-augmented systems LangChain is a powerful framework It lets you build retrieval-augmented systems.\n",
    "    LangChain is a powerful framework. It lets you build retrieval-augmented systems.\\n\\n\n",
    "    SpaCy helps with sentence segmentation. It knows linguistics.\\n\\n\n",
    "    Paris is the capital of France. Itâ€™s known for the Eiffel Tower.\n",
    "\"\"\"\n",
    "\n",
    "splitter = SpacyTextSplitter( pipeline=\"en_core_web_sm\", strip_whitespace=True,chunk_size=40,chunk_overlap=0)\n",
    "chunks = splitter.split_text(content)\n",
    "print(\"Chunks:\", chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9c7651",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "\n",
    "Welcome to the documentation. This is the intro text under the project.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This section gives an overview of the project goals and architecture.\n",
    "\n",
    "## Usage\n",
    "\n",
    "Here are usage instructions:\n",
    "\n",
    "- Install dependencies  \n",
    "- Run the main app  \n",
    "- Use flags and config\n",
    "\n",
    "### Advanced Usage\n",
    "\n",
    "More detailed options, flags, configurations, caveats.\n",
    "\n",
    "## Contributing\n",
    "\n",
    "Guidelines for contributing, how to submit pull requests, coding style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4309bef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown = \"\"\"\n",
    "# Project Title\n",
    "\n",
    "Welcome to the documentation. This is the intro text under the project.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This section gives an overview of the project goals and architecture.\n",
    "\n",
    "## Usage\n",
    "\n",
    "Here are usage instructions:\n",
    "\n",
    "- Install dependencies  \n",
    "- Run the main app  \n",
    "- Use flags and config\n",
    "\n",
    "### Advanced Usage\n",
    "\n",
    "More detailed options, flags, configurations, caveats.\n",
    "\n",
    "## Contributing\n",
    "\n",
    "Guidelines for contributing, how to submit pull requests, coding style.\n",
    "\n",
    "#### Final Notes\n",
    "\n",
    "Final Notes to be written here .\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d43d6fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown Chunks: [Document(metadata={'H1': 'Project Title'}, page_content='Welcome to the documentation. This is the intro text under the project.'), Document(metadata={'H1': 'Project Title', 'H2': 'Overview'}, page_content='This section gives an overview of the project goals and architecture.'), Document(metadata={'H1': 'Project Title', 'H2': 'Usage'}, page_content='Here are usage instructions:  \\n- Install dependencies\\n- Run the main app\\n- Use flags and config'), Document(metadata={'H1': 'Project Title', 'H2': 'Usage', 'H3': 'Advanced Usage'}, page_content='More detailed options, flags, configurations, caveats.'), Document(metadata={'H1': 'Project Title', 'H2': 'Contributing'}, page_content='Guidelines for contributing, how to submit pull requests, coding style.  \\n#### Final Notes  \\nFinal Notes to be written here .')]\n"
     ]
    }
   ],
   "source": [
    "splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=[(\"#\", \"H1\"), (\"##\", \"H2\"), (\"###\", \"H3\")],\n",
    "    return_each_line=False,\n",
    "    strip_headers=True,\n",
    ")\n",
    "docs = splitter.split_text(markdown)\n",
    "print(\"Markdown Chunks:\", docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4f1917b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown Chunks: [Document(metadata={'H1': 'Project Title'}, page_content='Welcome to the documentation. This is the intro text under the project.'), Document(metadata={'H1': 'Project Title', 'H2': 'Overview'}, page_content='This section gives an overview of the project goals and architecture.'), Document(metadata={'H1': 'Project Title', 'H2': 'Usage'}, page_content='Here are usage instructions:  \\n- Install dependencies\\n- Run the main app\\n- Use flags and config'), Document(metadata={'H1': 'Project Title', 'H2': 'Usage', 'H3': 'Advanced Usage'}, page_content='More detailed options, flags, configurations, caveats.'), Document(metadata={'H1': 'Project Title', 'H2': 'Contributing'}, page_content='Guidelines for contributing, how to submit pull requests, coding style.'), Document(metadata={'H1': 'Project Title', 'H2': 'Contributing', 'H4': 'Final Notes'}, page_content='Final Notes to be written here .')]\n"
     ]
    }
   ],
   "source": [
    "splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=[(\"#\", \"H1\"), (\"##\", \"H2\"), (\"###\", \"H3\"), (\"####\", \"H4\")],\n",
    "    return_each_line=False,\n",
    "    strip_headers=True,\n",
    ")\n",
    "docs = splitter.split_text(markdown)\n",
    "print(\"Markdown Chunks:\", docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "deedbc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown Chunks: [Document(metadata={'H1': 'Project Title'}, page_content='Welcome to the documentation. This is the intro text under the project.'), Document(metadata={'H1': 'Project Title', 'H2': 'Overview'}, page_content='This section gives an overview of the project goals and architecture.'), Document(metadata={'H1': 'Project Title', 'H2': 'Usage'}, page_content='Here are usage instructions:'), Document(metadata={'H1': 'Project Title', 'H2': 'Usage'}, page_content='- Install dependencies\\n- Run the main app\\n- Use flags and config'), Document(metadata={'H1': 'Project Title', 'H2': 'Usage', 'H3': 'Advanced Usage'}, page_content='More detailed options, flags, configurations, caveats.'), Document(metadata={'H1': 'Project Title', 'H2': 'Contributing'}, page_content='Guidelines for contributing, how to submit pull requests, coding style.'), Document(metadata={'H1': 'Project Title', 'H2': 'Contributing', 'H4': 'Final Notes'}, page_content='Final Notes to be written here .')]\n"
     ]
    }
   ],
   "source": [
    "splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=[(\"#\", \"H1\"), (\"##\", \"H2\"), (\"###\", \"H3\"), (\"####\", \"H4\")],\n",
    "    return_each_line=True,\n",
    "    strip_headers=True,\n",
    ")\n",
    "docs = splitter.split_text(markdown)\n",
    "print(\"Markdown Chunks:\", docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0d06328d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown Chunks: [Document(metadata={'H1': 'Project Title'}, page_content='# Project Title'), Document(metadata={'H1': 'Project Title'}, page_content='Welcome to the documentation. This is the intro text under the project.'), Document(metadata={'H1': 'Project Title', 'H2': 'Overview'}, page_content='## Overview'), Document(metadata={'H1': 'Project Title', 'H2': 'Overview'}, page_content='This section gives an overview of the project goals and architecture.'), Document(metadata={'H1': 'Project Title', 'H2': 'Usage'}, page_content='## Usage'), Document(metadata={'H1': 'Project Title', 'H2': 'Usage'}, page_content='Here are usage instructions:'), Document(metadata={'H1': 'Project Title', 'H2': 'Usage'}, page_content='- Install dependencies\\n- Run the main app\\n- Use flags and config'), Document(metadata={'H1': 'Project Title', 'H2': 'Usage', 'H3': 'Advanced Usage'}, page_content='### Advanced Usage'), Document(metadata={'H1': 'Project Title', 'H2': 'Usage', 'H3': 'Advanced Usage'}, page_content='More detailed options, flags, configurations, caveats.'), Document(metadata={'H1': 'Project Title', 'H2': 'Contributing'}, page_content='## Contributing'), Document(metadata={'H1': 'Project Title', 'H2': 'Contributing'}, page_content='Guidelines for contributing, how to submit pull requests, coding style.'), Document(metadata={'H1': 'Project Title', 'H2': 'Contributing', 'H4': 'Final Notes'}, page_content='#### Final Notes'), Document(metadata={'H1': 'Project Title', 'H2': 'Contributing', 'H4': 'Final Notes'}, page_content='Final Notes to be written here .')]\n"
     ]
    }
   ],
   "source": [
    "splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=[(\"#\", \"H1\"), (\"##\", \"H2\"), (\"###\", \"H3\"), (\"####\", \"H4\")],\n",
    "    return_each_line=True,\n",
    "    strip_headers=False,\n",
    ")\n",
    "docs = splitter.split_text(markdown)\n",
    "print(\"Markdown Chunks:\", docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "712c2379",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"\"\"\n",
    "<html>\n",
    "  <body>\n",
    "    <h1>Introduction</h1>\n",
    "    <p>Welcome to our site.</p>\n",
    "    <h2>Background</h2>\n",
    "    <p>Some background content.</p>\n",
    "    <h1>Conclusion</h1>\n",
    "    <p>Final thoughts go here.</p>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5b4bad92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML Chunks: [Document(metadata={'Main': 'Introduction'}, page_content='Introduction'), Document(metadata={'Main': 'Introduction'}, page_content='Welcome to our site.'), Document(metadata={'Main': 'Introduction', 'Sub': 'Background'}, page_content='Background'), Document(metadata={'Main': 'Introduction', 'Sub': 'Background'}, page_content='Some background content.'), Document(metadata={'Main': 'Conclusion'}, page_content='Conclusion'), Document(metadata={'Main': 'Conclusion'}, page_content='Final thoughts go here.')]\n"
     ]
    }
   ],
   "source": [
    "headers_to_split_on = [(\"h1\", \"Main\"), (\"h2\", \"Sub\")]\n",
    "\n",
    "splitter = HTMLHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on,\n",
    "    return_each_element=False\n",
    ")\n",
    "docs = splitter.split_text(html)\n",
    "print(\"HTML Chunks:\", docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
