{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataStream API\n",
    "The DataStream API is designed for low-level control of real-time stream processing. \n",
    "It allows developers to work with unbounded streams of data by providing event-by-event handling and stateful transformations.\n",
    "\n",
    "#### Core Features:\n",
    "\n",
    "1. Event-Driven Processing: Operates on a stream of individual events.\n",
    "2. Custom Logic: Supports custom business logic with functions like ProcessFunction.\n",
    "3. State Management: Explicit support for managing state, both keyed and operator states.\n",
    "4. Time Semantics: Offers event time, processing time, and watermarking for precise control over time-based operations.\n",
    "5. Windowing: Aggregates data over time-based or count-based windows.\n",
    "6. Transformations: Includes operations like map, filter, flatMap, keyBy, reduce, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scenarios on When to Use DataStream API\n",
    "\n",
    "- Complex Event Processing: When you need fine-grained control over events (e.g., detecting patterns like fraud or anomalies).\n",
    "- Custom Logic: If your use case involves non-relational transformations (e.g., custom calculations or iterative processing).\n",
    "- IoT Monitoring: Process sensor data where events are key-based and require specific windowing logic.\n",
    "\n",
    "#### Scenarios on When to Use Table API\n",
    "\n",
    "- Relational Workloads: For tasks like aggregations, joins, and filtering similar to SQL.\n",
    "- Unified Batch and Stream Processing: When processing both historical data and real-time events in the same application.\n",
    "- Business Reporting: For real-time dashboards and analytics requiring declarative queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.common.typeinfo import Types\n",
    "from pyflink.datastream.connectors.kafka import FlinkKafkaConsumer\n",
    "from pyflink.datastream.formats.json import JsonRowDeserializationSchema\n",
    "from pyflink.datastream.connectors.file_system import FileSink, OutputFileConfig\n",
    "from pyflink.common.serialization import Encoder\n",
    "from pyflink.table.expressions import col\n",
    "from pyflink.common import Types\n",
    "from pyflink.table import StreamTableEnvironment, EnvironmentSettings\n",
    "from pyflink.common import WatermarkStrategy, Duration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StreamExecutionEnvironment.get_execution_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = env.from_collection(collection=[(1,'praveen'),(2,'chinna'),(3,'reddy')],type_info=Types.ROW([Types.INT(),Types.STRING()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.add_jars('file:///Users/praveenreddy/FFlink/flink-sql-connector-kafka-1.17.0.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "deserialization_schema = JsonRowDeserializationSchema.builder()\\\n",
    "    .type_info(Types.ROW_NAMED(\n",
    "        [\"key\", \"data\"],  # Field names must match JSON keys\n",
    "        [Types.STRING(), Types.STRING()]  # Field types\n",
    "    )).build()\n",
    "\n",
    "\n",
    "\n",
    "kafka_consumer = FlinkKafkaConsumer(\n",
    "    topics='test_topic',\n",
    "    deserialization_schema=deserialization_schema,\n",
    "    properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'praveen_group_id_2'})\n",
    "\n",
    "ds = env.add_source(kafka_consumer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(key='0', data='Hello')\n",
      "Row(key='1', data='Namaste')\n",
      "Row(key='2', data='Good Day')\n",
      "Row(key='3', data='Hi')\n",
      "Row(key='4', data='Good Day')\n",
      "Row(key='5', data='Good Day')\n",
      "Row(key='6', data='Hello')\n",
      "Row(key='7', data='Good Day')\n",
      "Row(key='8', data='Hi')\n",
      "Row(key='9', data='Namaste')\n",
      "Row(key='0', data='Good Day')\n",
      "Row(key='1', data='Hi')\n",
      "Row(key='2', data='Hi')\n",
      "Row(key='3', data='Namaste')\n",
      "Row(key='4', data='Good Day')\n",
      "Row(key='5', data='Hi')\n",
      "Row(key='6', data='Namaste')\n",
      "Row(key='7', data='Good Day')\n",
      "Row(key='8', data='Good Day')\n",
      "Row(key='9', data='Good Day')\n",
      "Row(key='0', data='Hi')\n",
      "Row(key='1', data='Namaste')\n",
      "Row(key='2', data='Hi')\n",
      "Row(key='3', data='Hi')\n",
      "Row(key='4', data='Hello')\n",
      "Row(key='5', data='Namaste')\n",
      "Row(key='6', data='Namaste')\n",
      "Row(key='7', data='Hello')\n",
      "Row(key='8', data='Hi')\n",
      "Row(key='9', data='Hello')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/praveenreddy/FFlink/Flink_Work/myenv/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/praveenreddy/FFlink/Flink_Work/myenv/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1217, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ds\u001b[38;5;241m.\u001b[39mexecute_and_collect() \u001b[38;5;28;01mas\u001b[39;00m results:\n\u001b[0;32m----> 2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/FFlink/Flink_Work/myenv/lib/python3.11/site-packages/pyflink/datastream/data_stream.py:2920\u001b[0m, in \u001b[0;36mCloseableIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2919\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 2920\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/FFlink/Flink_Work/myenv/lib/python3.11/site-packages/pyflink/datastream/data_stream.py:2929\u001b[0m, in \u001b[0;36mCloseableIterator.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2928\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 2929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_j_closeable_iterator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhasNext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   2930\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo more data.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   2931\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_python_obj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_j_closeable_iterator\u001b[38;5;241m.\u001b[39mnext(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_type_info)\n",
      "File \u001b[0;32m~/FFlink/Flink_Work/myenv/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/FFlink/Flink_Work/myenv/lib/python3.11/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/FFlink/Flink_Work/myenv/lib/python3.11/site-packages/py4j/java_gateway.py:1217\u001b[0m, in \u001b[0;36mGatewayConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(\n\u001b[1;32m   1214\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while sending\u001b[39m\u001b[38;5;124m\"\u001b[39m, e, proto\u001b[38;5;241m.\u001b[39mERROR_ON_SEND)\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1217\u001b[0m     answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m   1218\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m answer\u001b[38;5;241m.\u001b[39mstartswith(proto\u001b[38;5;241m.\u001b[39mRETURN_MESSAGE):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with ds.execute_and_collect() as results:\n",
    "    for result in results:\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar code for KafkaProducer !! -- Just the Class Names vary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_producer = FlinkKafkaProducer(\n",
    "    topic='test_sink_topic',\n",
    "    serialization_schema=serialization_schema,\n",
    "    producer_config={'bootstrap.servers': 'localhost:9092', 'group.id': 'test_group'})\n",
    "\n",
    "ds.add_sink(kafka_producer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map(lambda row: f\"{row.key},{row.data}\", output_type=Types.STRING())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m file_sink \u001b[38;5;241m=\u001b[39m FileSink \\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mfor_row_format(output_path, Encoder\u001b[38;5;241m.\u001b[39msimple_string_encoder()) \\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mwith_output_file_config(OutputFileConfig\u001b[38;5;241m.\u001b[39mbuilder()\u001b[38;5;241m.\u001b[39mwith_part_prefix(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpre\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mwith_part_suffix(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuf\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mbuild()) \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mbuild()\n\u001b[1;32m      7\u001b[0m ds\u001b[38;5;241m.\u001b[39msink_to(file_sink)\n\u001b[0;32m----> 9\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKafka to FileSink\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/FFlink/Flink_Work/myenv/lib/python3.11/site-packages/pyflink/datastream/stream_execution_environment.py:824\u001b[0m, in \u001b[0;36mStreamExecutionEnvironment.execute\u001b[0;34m(self, job_name)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;124;03mTriggers the program execution. The environment will execute all parts of\u001b[39;00m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;124;03mthe program that have resulted in a \"sink\" operation. Sink operations are\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;124;03m:return: The result of the job execution, containing elapsed time and accumulators.\u001b[39;00m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    823\u001b[0m j_stream_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_stream_graph(clear_transformations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, job_name\u001b[38;5;241m=\u001b[39mjob_name)\n\u001b[0;32m--> 824\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m JobExecutionResult(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_j_stream_execution_environment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mj_stream_graph\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/FFlink/Flink_Work/myenv/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/FFlink/Flink_Work/myenv/lib/python3.11/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/FFlink/Flink_Work/myenv/lib/python3.11/site-packages/py4j/java_gateway.py:1217\u001b[0m, in \u001b[0;36mGatewayConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(\n\u001b[1;32m   1214\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while sending\u001b[39m\u001b[38;5;124m\"\u001b[39m, e, proto\u001b[38;5;241m.\u001b[39mERROR_ON_SEND)\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1217\u001b[0m     answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m   1218\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m answer\u001b[38;5;241m.\u001b[39mstartswith(proto\u001b[38;5;241m.\u001b[39mRETURN_MESSAGE):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output_path = '/Users/praveenreddy/FFlink/Flink_Work'\n",
    "file_sink = FileSink \\\n",
    "    .for_row_format(output_path, Encoder.simple_string_encoder()) \\\n",
    "    .with_output_file_config(OutputFileConfig.builder().with_part_prefix('pre').with_part_suffix('suf').build()) \\\n",
    "    .build()\n",
    "\n",
    "ds.sink_to(file_sink)\n",
    "\n",
    "env.execute(\"Kafka to FileSink\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2035.executeInsert.\n: org.apache.flink.table.api.ValidationException: Unable to create a sink for writing table 'default_catalog.default_database.file_sink'.\n\nTable options are:\n\n'connector'='print'\n'format'='csv'\n'path'='output'\n\tat org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:338)\n\tat org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:450)\n\tat org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:227)\n\tat org.apache.flink.table.planner.delegation.PlannerBase.$anonfun$translate$1(PlannerBase.scala:177)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)\n\tat scala.collection.Iterator.foreach(Iterator.scala:937)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:937)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1425)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:70)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:69)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:233)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:226)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:177)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1308)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:874)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1107)\n\tat org.apache.flink.table.api.internal.TablePipelineImpl.execute(TablePipelineImpl.java:59)\n\tat org.apache.flink.table.api.Table.executeInsert(Table.java:1074)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)\n\tat org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:842)\nCaused by: org.apache.flink.table.api.ValidationException: Unsupported options found for 'print'.\n\nUnsupported options:\n\nformat\npath\n\nSupported options:\n\nconnector\nprint-identifier\nproperty-version\nscan.watermark.alignment.group\nscan.watermark.alignment.max-drift\nscan.watermark.alignment.update-interval\nscan.watermark.emit.strategy\nscan.watermark.idle-timeout\nsink.parallelism\nstandard-error\n\tat org.apache.flink.table.factories.FactoryUtil.validateUnconsumedKeys(FactoryUtil.java:710)\n\tat org.apache.flink.table.factories.FactoryUtil$FactoryHelper.validate(FactoryUtil.java:1009)\n\tat org.apache.flink.connector.print.table.PrintTableSinkFactory.createDynamicTableSink(PrintTableSinkFactory.java:88)\n\tat org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:335)\n\t... 30 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 35\u001b[0m\n\u001b[1;32m     22\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/praveenreddy/FFlink/Flink_Work/output_data\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     24\u001b[0m table_env\u001b[38;5;241m.\u001b[39mexecute_sql(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124m    CREATE TABLE file_sink (\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124m        key STRING,\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124m    )\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_insert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfile_sink\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mwait()\n\u001b[1;32m     37\u001b[0m csv_table \u001b[38;5;241m=\u001b[39m table_env\u001b[38;5;241m.\u001b[39mfrom_path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_sink\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m csv_table\u001b[38;5;241m.\u001b[39mexecute()\u001b[38;5;241m.\u001b[39mprint()\n",
      "File \u001b[0;32m~/FFlink/Flink_Work/myenv/lib/python3.11/site-packages/pyflink/table/table.py:1045\u001b[0m, in \u001b[0;36mTable.execute_insert\u001b[0;34m(self, table_path_or_descriptor, overwrite)\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_t_env\u001b[38;5;241m.\u001b[39m_before_execute()\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(table_path_or_descriptor, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 1045\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TableResult(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_j_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecuteInsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable_path_or_descriptor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1047\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TableResult(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_j_table\u001b[38;5;241m.\u001b[39mexecuteInsert(\n\u001b[1;32m   1048\u001b[0m         table_path_or_descriptor\u001b[38;5;241m.\u001b[39m_j_table_descriptor, overwrite))\n",
      "File \u001b[0;32m~/FFlink/Flink_Work/myenv/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/FFlink/Flink_Work/myenv/lib/python3.11/site-packages/pyflink/util/exceptions.py:146\u001b[0m, in \u001b[0;36mcapture_java_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyflink\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjava_gateway\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_gateway\n",
      "File \u001b[0;32m~/FFlink/Flink_Work/myenv/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2035.executeInsert.\n: org.apache.flink.table.api.ValidationException: Unable to create a sink for writing table 'default_catalog.default_database.file_sink'.\n\nTable options are:\n\n'connector'='print'\n'format'='csv'\n'path'='output'\n\tat org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:338)\n\tat org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:450)\n\tat org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:227)\n\tat org.apache.flink.table.planner.delegation.PlannerBase.$anonfun$translate$1(PlannerBase.scala:177)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)\n\tat scala.collection.Iterator.foreach(Iterator.scala:937)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:937)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1425)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:70)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:69)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:233)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:226)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:177)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1308)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:874)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1107)\n\tat org.apache.flink.table.api.internal.TablePipelineImpl.execute(TablePipelineImpl.java:59)\n\tat org.apache.flink.table.api.Table.executeInsert(Table.java:1074)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)\n\tat org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:842)\nCaused by: org.apache.flink.table.api.ValidationException: Unsupported options found for 'print'.\n\nUnsupported options:\n\nformat\npath\n\nSupported options:\n\nconnector\nprint-identifier\nproperty-version\nscan.watermark.alignment.group\nscan.watermark.alignment.max-drift\nscan.watermark.alignment.update-interval\nscan.watermark.emit.strategy\nscan.watermark.idle-timeout\nsink.parallelism\nstandard-error\n\tat org.apache.flink.table.factories.FactoryUtil.validateUnconsumedKeys(FactoryUtil.java:710)\n\tat org.apache.flink.table.factories.FactoryUtil$FactoryHelper.validate(FactoryUtil.java:1009)\n\tat org.apache.flink.connector.print.table.PrintTableSinkFactory.createDynamicTableSink(PrintTableSinkFactory.java:88)\n\tat org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:335)\n\t... 30 more\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "env = StreamExecutionEnvironment.get_execution_environment()\n",
    "env.add_jars('file:///Users/praveenreddy/FFlink/flink-sql-connector-kafka-1.17.0.jar')\n",
    "env_settings = EnvironmentSettings.new_instance().in_streaming_mode().build()\n",
    "table_env = StreamTableEnvironment.create(stream_execution_environment=env, environment_settings=env_settings)\n",
    "\n",
    "deserialization_schema = JsonRowDeserializationSchema.builder()\\\n",
    "    .type_info(Types.ROW_NAMED(\n",
    "        [\"key\", \"data\"],  # Field names must match JSON keys\n",
    "        [Types.STRING(), Types.STRING()]  # Field types\n",
    "    )).build()\n",
    "\n",
    "kafka_consumer = FlinkKafkaConsumer(\n",
    "    topics='test_topic',\n",
    "    deserialization_schema=deserialization_schema,\n",
    "    properties={'bootstrap.servers': 'localhost:9092', 'group.id': 'praveen_group_id_3'})\n",
    "\n",
    "ds = env.add_source(kafka_consumer)\n",
    "\n",
    "# Convert DataStream to Table\n",
    "table = table_env.from_data_stream(ds, col(\"key\"), col(\"data\"))\n",
    "\n",
    "output_path = '/Users/praveenreddy/FFlink/Flink_Work/output_data'\n",
    "\n",
    "table_env.execute_sql(f\"\"\"\n",
    "    CREATE TABLE file_sink (\n",
    "        key STRING,\n",
    "        data STRING\n",
    "    ) WITH (\n",
    "        'connector' = 'filesystem',\n",
    "        'path' = 'output',\n",
    "        'format' = 'csv'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "table.execute_insert(\"file_sink\").wait()\n",
    "\n",
    "csv_table = table_env.from_path(\"file_sink\")\n",
    "\n",
    "env.execute(\"DataStream to Table API Job\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread read_grpc_client_inputs:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/praveenreddy/FFlink/Flink_Work/myenv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/praveenreddy/FFlink/Flink_Work/myenv/lib/python3.11/site-packages/apache_beam/runners/worker/data_plane.py\", line 669, in <lambda>\n",
      "    target=lambda: self._read_inputs(elements_iterator),\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/praveenreddy/FFlink/Flink_Work/myenv/lib/python3.11/site-packages/apache_beam/runners/worker/data_plane.py\", line 652, in _read_inputs\n",
      "    for elements in elements_iterator:\n",
      "  File \"/Users/praveenreddy/FFlink/Flink_Work/myenv/lib/python3.11/site-packages/grpc/_channel.py\", line 543, in __next__\n",
      "    return self._next()\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/praveenreddy/FFlink/Flink_Work/myenv/lib/python3.11/site-packages/grpc/_channel.py\", line 969, in _next\n",
      "    raise self\n",
      "grpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:\n",
      "\tstatus = StatusCode.CANCELLED\n",
      "\tdetails = \"Multiplexer hanging up\"\n",
      "\tdebug_error_string = \"UNKNOWN:Error received from peer ipv6:%5B::1%5D:58271 {grpc_message:\"Multiplexer hanging up\", grpc_status:1, created_time:\"2024-12-16T17:15:14.66683+05:30\"}\"\n",
      ">\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5> Hello\n",
      "5> world\n",
      "6> Apache\n",
      "6> Flink\n",
      "6> is\n",
      "6> great\n",
      "1> DataStream\n",
      "1> API\n",
      "1> is\n",
      "1> powerful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyflink.common.job_execution_result.JobExecutionResult at 0x10584f3d0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "\n",
    "env = StreamExecutionEnvironment.get_execution_environment()\n",
    "\n",
    "# Step 2: Read the file\n",
    "file_path = \"input.txt\"\n",
    "ds = env.read_text_file(file_path)\n",
    "\n",
    "ds = ds.flat_map(lambda line: line.split(\" \"))\n",
    "\n",
    "ds.print()\n",
    "\n",
    "env.execute(\"Read DataStream from File\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread read_grpc_client_inputs:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/praveenreddy/FFlink/Flink_Work/myenv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/praveenreddy/FFlink/Flink_Work/myenv/lib/python3.11/site-packages/apache_beam/runners/worker/data_plane.py\", line 669, in <lambda>\n",
      "    target=lambda: self._read_inputs(elements_iterator),\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/praveenreddy/FFlink/Flink_Work/myenv/lib/python3.11/site-packages/apache_beam/runners/worker/data_plane.py\", line 652, in _read_inputs\n",
      "    for elements in elements_iterator:\n",
      "  File \"/Users/praveenreddy/FFlink/Flink_Work/myenv/lib/python3.11/site-packages/grpc/_channel.py\", line 543, in __next__\n",
      "    return self._next()\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/praveenreddy/FFlink/Flink_Work/myenv/lib/python3.11/site-packages/grpc/_channel.py\", line 969, in _next\n",
      "    raise self\n",
      "grpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:\n",
      "\tstatus = StatusCode.CANCELLED\n",
      "\tdetails = \"Multiplexer hanging up\"\n",
      "\tdebug_error_string = \"UNKNOWN:Error received from peer ipv6:%5B::1%5D:58416 {created_time:\"2024-12-16T17:24:06.589284+05:30\", grpc_status:1, grpc_message:\"Multiplexer hanging up\"}\"\n",
      ">\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1> (4,chinnareddy,60)\n",
      "7> (3,reddy,55)\n",
      "4> (1,praveen,29)\n",
      "5> (2,chinna,19)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyflink.common.job_execution_result.JobExecutionResult at 0x15d5029d0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = StreamExecutionEnvironment.get_execution_environment()\n",
    "\n",
    "csv_path = \"input_csv.csv\"\n",
    "ds = env.read_text_file(csv_path)\n",
    "\n",
    "# Parse the CSV content into a tuple DataStream\n",
    "parsed_ds = ds.map(lambda line: line.split(',')) \\\n",
    "              .map(lambda fields: (int(fields[0]), fields[1], int(fields[2])),\n",
    "                   output_type=Types.TUPLE([Types.INT(), Types.STRING(), Types.INT()]))\n",
    "\n",
    "# Print the parsed data to console\n",
    "parsed_ds.print()\n",
    "\n",
    "env.execute(\"Read CSV File\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windows in Apache Flink\n",
    "Windows are essential for processing infinite data streams because they break the stream into finite \"buckets\". These buckets allow Flink to perform computations on smaller chunks of the data stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyed and Non-Keyed Windows?\n",
    "#### Keyed Windows:\n",
    "\n",
    "You divide the stream into logical sub-streams (called keyed streams) based on a key.\n",
    "\n",
    "Use keyBy(...) to assign a key.\n",
    "\n",
    "Allows parallel processing since each key is processed independently.\n",
    "\n",
    "Example:\n",
    "stream.key_by(lambda x: x.user_id).window(...).reduce(...)  # Keyed by user_id\n",
    "\n",
    "\n",
    "#### Non-Keyed Windows:\n",
    "The entire stream is treated as a single unit.\n",
    "\n",
    "Use windowAll(...) instead of window(...).\n",
    "\n",
    "All processing happens in a single task with no parallelism.\n",
    "\n",
    "Example:\n",
    "stream.window_all(...).reduce(...)  # Non-keyed processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How is Windows Creation and Removal works ?:**\n",
    "\n",
    "A window is created when the first element for that window arrives.\n",
    "\n",
    "The window is removed after its end timestamp and an additional \"lateness allowance\" (if specified).\n",
    "\n",
    "**Example:**\n",
    "A tumbling window of 5 minutes starts at 12:00 and ends at 12:05.\n",
    "If \"lateness\" is set to 1 minute, the window is cleared at 12:06."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Core Components of a Window:**\n",
    "\n",
    "Window Assigners: Define how elements are grouped into windows (e.g., time-based or session-based).\n",
    "\n",
    "Trigger: Defines when a window is \"ready\" for computation (e.g., after 10 elements or when the watermark reaches a specific time).\n",
    "\n",
    "Function: Specifies the computation to apply (e.g., sum, average, or a custom function).\n",
    "\n",
    "Evictor (Optional): Removes elements from the window before or after applying the function. [not supported in Python API]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Window Assigners**\n",
    "Assigners group stream elements into windows:\n",
    "\n",
    "``Tumbling Windows: Fixed-size, non-overlapping windows.``\n",
    "\n",
    "Example: Windows of 5 seconds (12:00 to 12:05, 12:05 to 12:10, etc.).\n",
    "\n",
    "``Sliding Windows: Fixed-size, overlapping windows with a defined \"slide interval\".``\n",
    "\n",
    "Example: 5-second windows sliding every 2 seconds (overlap occurs).\n",
    "\n",
    "``Session Windows: Windows with gaps of inactivity. A new window starts after a period of no data.``\n",
    "\n",
    "``Global Windows: Contains all data without grouping by time (manual control needed for triggers).``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By default, GlobalWindows does not come with a trigger. Since it encompasses all elements into a single window, you must explicitly define a trigger to decide when the window should be processed\n",
    "\n",
    "- Sliding and tumbling windows have built-in triggers based on the time semantics you specify (event-time or processing-time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyflink.datastream import StreamExecutionEnvironment, TimeCharacteristic\n",
    "from pyflink.common.typeinfo import Types\n",
    "from pyflink.datastream.window import TumblingEventTimeWindows, SlidingEventTimeWindows, GlobalWindows\n",
    "from pyflink.common.time import Time\n",
    "from pyflink.datastream.functions import KeySelector, ReduceFunction\n",
    "from pyflink.common import WatermarkStrategy,Encoder\n",
    "import time\n",
    "\n",
    "from pyflink.common import WatermarkStrategy, Duration\n",
    "from pyflink.common.watermark_strategy import TimestampAssigner\n",
    "from pyflink.datastream.functions import ProcessWindowFunction\n",
    "\n",
    "from pyflink.table import StreamTableEnvironment, DataTypes\n",
    "from pyflink.table.descriptors import Schema\n",
    "from pyflink.common.typeinfo import Types\n",
    "from pyflink.datastream.connectors.file_system import FileSink\n",
    "\n",
    "from pyflink.datastream.functions import ProcessFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StreamExecutionEnvironment.get_execution_environment()\n",
    "env.set_parallelism(1)  # Set parallelism to 1 for simplicity\n",
    "env.set_stream_time_characteristic(TimeCharacteristic.EventTime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = int(time.time() * 1000)  # Current time in milliseconds\n",
    "data = [\n",
    "    (current_time - 7000, 10),  # Event 7 seconds ago\n",
    "    (current_time - 6000, 20),  # Event 6 seconds ago\n",
    "    (current_time - 5000, 30),  # Event 5 seconds ago\n",
    "    (current_time - 4000, 40),  # Event 4 seconds ago\n",
    "    (current_time - 3000, 50),  # Event 3 seconds ago\n",
    "    (current_time - 2000, 60),  # Event 2 seconds ago\n",
    "    (current_time - 1000, 70)   # Event 1 second ago\n",
    "]\n",
    "\n",
    "data_stream = env.from_collection(\n",
    "    data,\n",
    "    type_info=Types.TUPLE([Types.LONG(), Types.INT()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom key selector\n",
    "class EvenOddKeySelector(KeySelector):\n",
    "    def get_key(self, value):\n",
    "        return value[1] % 2  # Key by even (0) or odd (1) based on payload\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = [\n",
    "    (timestamp_1, 10),  (timestamp_2, 20),  (timestamp_3, 30) ]\n",
    "\n",
    "reduce:\n",
    "\n",
    "value1 = (timestamp_1, 10)\n",
    "\n",
    "value2 = (timestamp_2, 20)\n",
    "\n",
    "Result: (timestamp_1, 10 + 20) = (timestamp_1, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom reduce function\n",
    "class SumReduceFunction(ReduceFunction):\n",
    "    def reduce(self, value1, value2):\n",
    "        return (value1[0], value1[1] + value2[1])  # Sum payloads, keep the earliest timestamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FirstElementTimestampAssigner(TimestampAssigner):\n",
    "    def extract_timestamp(self, value, record_timestamp):\n",
    "        return value[0]  # Extract timestamp from the first field of the incoming event\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Allows events to arrive up to 20 seconds late compared to the current watermark (i.e., out-of-order events are tolerated for 20 seconds\n",
    "2. The FirstElementTimestampAssigner uses the first field of each event (e.g., a timestamp) as the event's timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the watermark strategy\n",
    "watermark_strategy = WatermarkStrategy.for_bounded_out_of_orderness(Duration.of_seconds(20)) \\\n",
    "    .with_timestamp_assigner(FirstElementTimestampAssigner()) # Extract timestamp from the first field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign timestamps and watermarks\n",
    "timestamped_stream = data_stream.assign_timestamps_and_watermarks(watermark_strategy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1734442493639,10)\n",
      "(1734442494639,90)\n",
      "(1734442497639,180)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyflink.common.job_execution_result.JobExecutionResult at 0x15cfd8950>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windowed_stream = (\n",
    "    timestamped_stream\n",
    "    .key_by(EvenOddKeySelector())  # Key by even/odd payload\n",
    "    .window(TumblingEventTimeWindows.of(Time.seconds(3)))  # Tumbling window of 3 seconds + 20 seconds of late arriving /out of order records \n",
    "    .reduce(SumReduceFunction())  # Sum values in each window\n",
    ")\n",
    "\n",
    "# Print the result\n",
    "windowed_stream.print()\n",
    "\n",
    "# Execute the program\n",
    "env.execute(\"Tumbling Window with Realistic Event Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1734442493639,30)\n",
      "(1734442493639,100)\n",
      "(1734442494639,200)\n",
      "(1734442496639,220)\n",
      "(1734442498639,130)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyflink.common.job_execution_result.JobExecutionResult at 0x15c873810>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Key by even/odd payload, create a sliding window of 5 seconds, sliding every 2 seconds\n",
    "windowed_stream = (\n",
    "    timestamped_stream\n",
    "    .key_by(EvenOddKeySelector())\n",
    "    .window(SlidingEventTimeWindows.of(Time.seconds(5), Time.seconds(2)))\n",
    "    .reduce(SumReduceFunction())\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "windowed_stream.print()\n",
    "\n",
    "# Execute the program\n",
    "env.execute(\"Sliding Window with Realistic Event Time\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flink's distributed nature means it operates across multiple worker nodes, and print statements often do not work the same way in a distributed streaming environment as they do in local testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyflink.common.job_execution_result.JobExecutionResult at 0x15cd78cd0>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "watermark_strategy = WatermarkStrategy.for_bounded_out_of_orderness(Duration.of_seconds(15))\n",
    "\n",
    "# Assign timestamps and watermarks\n",
    "timestamped_stream = timestamped_stream.assign_timestamps_and_watermarks(watermark_strategy)\n",
    "\n",
    "output_path = \"/Users/praveenreddy/FFlink/Flink_Work/global_window_output\"\n",
    "\n",
    "# Define a sink to write the output to a CSV file\n",
    "windowed_stream = (\n",
    "    timestamped_stream\n",
    "    .window_all(GlobalWindows())  \n",
    "    .reduce(SumReduceFunction())\n",
    ")\n",
    "windowed_stream.print()\n",
    "\n",
    "# Writing results to a CSV sink\n",
    "# windowed_stream.sink_to(\n",
    "#     FileSink.for_row_format(base_path=output_path,encoder=Encoder.simple_string_encoder()).build()\n",
    "# )\n",
    "\n",
    "# Execute the program\n",
    "env.execute(\"Global Window with Realistic Event Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIRE\n",
    "- Emits the current results of the window downstream but does not clear the window's state.\n",
    "\n",
    "PURGE\n",
    "- Clears the window's state without emitting any results downstream.\n",
    "\n",
    "FIRE_AND_PURGE\n",
    "- Emits the current results of the window and clears the window's state.\n",
    "\n",
    "CONTINUE\n",
    "- Does nothing and keeps the window's state intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1734477024357,10)\n",
      "(1734477024357,30)\n",
      "(1734477024357,60)\n",
      "(1734477024357,100)\n",
      "(1734477024357,150)\n",
      "(1734477024357,210)\n",
      "(1734477024357,280)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyflink.common.job_execution_result.JobExecutionResult at 0x15d9963d0>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.datastream.window import GlobalWindows, Trigger, TriggerResult\n",
    "from pyflink.common.time import Duration\n",
    "from pyflink.datastream.functions import ReduceFunction\n",
    "\n",
    "# Initialize environment\n",
    "env = StreamExecutionEnvironment.get_execution_environment()\n",
    "env.set_parallelism(1)\n",
    "\n",
    "# Define data\n",
    "import time\n",
    "current_time = int(time.time() * 1000)\n",
    "data = [\n",
    "    (current_time - 7000, 10),  # Event 7 seconds ago\n",
    "    (current_time - 6000, 20),  # Event 6 seconds ago\n",
    "    (current_time - 5000, 30),  # Event 5 seconds ago\n",
    "    (current_time - 4000, 40),  # Event 4 seconds ago\n",
    "    (current_time - 3000, 50),  # Event 3 seconds ago\n",
    "    (current_time - 2000, 60),  # Event 2 seconds ago\n",
    "    (current_time - 1000, 70)   # Event 1 second ago\n",
    "]\n",
    "# Create data stream\n",
    "data_stream = env.from_collection(\n",
    "    collection=data,\n",
    "    type_info=Types.TUPLE([Types.LONG(), Types.INT()])\n",
    ")\n",
    "\n",
    "# Define reduce function\n",
    "class SumReduceFunction(ReduceFunction):\n",
    "    def reduce(self, value1, value2):\n",
    "        return (value1[0], value1[1] + value2[1])\n",
    "\n",
    "from pyflink.datastream.window import Trigger, TriggerResult\n",
    "\n",
    "\n",
    "class CustomTrigger(Trigger):\n",
    "    def on_element(self, element, timestamp, window, ctx):\n",
    "        # This method is called every time a new element is added to the window.\n",
    "        return TriggerResult.FIRE\n",
    "#TriggerResult.PURGE:\n",
    "# Clear the state of the window without emitting results.\n",
    "\n",
    "#This method is invoked when a processing-time timer is triggered (based on wall-clock time).\n",
    "    def on_processing_time(self, time, window, ctx):\n",
    "        # Do nothing and wait for more elements or the next trigger condition\n",
    "        return TriggerResult.CONTINUE\n",
    "\n",
    "#This method is called when an event-time timer is triggered (based on the event-time watermark reaching a certain threshold).\n",
    "    def on_event_time(self, time, window, ctx):\n",
    "        # Do nothing and wait for more elements or the next trigger condition\n",
    "        return TriggerResult.CONTINUE\n",
    "\n",
    "#This method is used when windows are merged (e.g., in session windows or dynamic scenarios).\n",
    "    def on_merge(self, window, ctx):\n",
    "        # No merging logic required for GlobalWindows\n",
    "        pass\n",
    "\n",
    "#This method is called when the window expires (based on watermark + allowed lateness) or when explicitly cleared by the system or user logic.\n",
    "    def clear(self, window, ctx):\n",
    "        # Clear the state for the window\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define watermark strategy\n",
    "watermark_strategy = WatermarkStrategy.for_bounded_out_of_orderness(Duration.of_seconds(15)).with_timestamp_assigner(FirstElementTimestampAssigner()) \n",
    "\n",
    "# Assign timestamps and watermarks\n",
    "timestamped_stream = data_stream.assign_timestamps_and_watermarks(watermark_strategy)\n",
    "\n",
    "# Define global window with custom trigger\n",
    "windowed_stream = (\n",
    "    timestamped_stream\n",
    "    .window_all(GlobalWindows())\n",
    "    .trigger(CustomTrigger())\n",
    "    .reduce(SumReduceFunction())\n",
    ")\n",
    "\n",
    "# Debugging: Print results\n",
    "windowed_stream.print()\n",
    "\n",
    "# Execute program\n",
    "env.execute(\"Global Window with Realistic Event Time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = StreamExecutionEnvironment.get_execution_environment()\n",
    "env.set_parallelism(1)\n",
    "\n",
    "# Sensor data: (timestamp, sensor_id, temperature)\n",
    "import time\n",
    "current_time = int(time.time() * 1000)\n",
    "data = [\n",
    "    (current_time - 7000, \"Sensor-1\", 30.5),\n",
    "    (current_time - 6000, \"Sensor-1\", 32.0),\n",
    "    (current_time - 5000, \"Sensor-1\", 35.8),\n",
    "    (current_time - 4000, \"Sensor-2\", 29.2),\n",
    "    (current_time - 3000, \"Sensor-2\", 33.5),\n",
    "    (current_time - 2000, \"Sensor-1\", 37.1),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SensorTimestampAssigner(TimestampAssigner):\n",
    "    def extract_timestamp(self, value, record_timestamp):\n",
    "        return value[0]  # Timestamp is the first field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxTemperatureFunction(ProcessWindowFunction):\n",
    "    def process(self, key, context, elements):\n",
    "        max_temp = float('-inf')\n",
    "        max_record = None\n",
    "\n",
    "        # Find the maximum temperature in the window\n",
    "        for element in elements:\n",
    "            if element[2] > max_temp:\n",
    "                max_temp = element[2]\n",
    "                max_record = element\n",
    "\n",
    "        window_start = context.window().start\n",
    "        window_end = context.window().end\n",
    "\n",
    "        # Collect results as an iterable (list)\n",
    "        result = [\n",
    "            f\"Sensor: {key}, Window: [{window_start}, {window_end}), Max Temp: {max_temp}\"\n",
    "        ]\n",
    "\n",
    "        # Emit the result\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "watermark_strategy = WatermarkStrategy.for_bounded_out_of_orderness(Duration.of_seconds(2)) \\\n",
    "    .with_timestamp_assigner(SensorTimestampAssigner())\n",
    "\n",
    "data_stream = env.from_collection(data)\n",
    "timestamped_stream = data_stream.assign_timestamps_and_watermarks(watermark_strategy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor: Sensor-1, Window: [1734479245000, 1734479250000), Max Temp: 32.0\n",
      "Sensor: Sensor-2, Window: [1734479250000, 1734479255000), Max Temp: 33.5\n",
      "Sensor: Sensor-1, Window: [1734479250000, 1734479255000), Max Temp: 37.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyflink.common.job_execution_result.JobExecutionResult at 0x15c8ce9d0>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windowed_stream = (\n",
    "    timestamped_stream\n",
    "    .key_by(lambda x: x[1])  # Key by sensor ID\n",
    "    .window(TumblingEventTimeWindows.of(Time.seconds(5)))\n",
    "    .process(MaxTemperatureFunction())\n",
    ")\n",
    "\n",
    "windowed_stream.print()\n",
    "env.execute(\"Max Temperature with ProcessWindowFunction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StreamExecutionEnvironment.get_execution_environment()\n",
    "env.set_parallelism(1)\n",
    "\n",
    "# Set up Table Environment\n",
    "t_env = StreamTableEnvironment.create(env)\n",
    "\n",
    "# Define output path\n",
    "output_path = \"/Users/praveenreddy/FFlink/Flink_Work/real_time_output\"\n",
    "\n",
    "# Define some sample data\n",
    "current_time = int(time.time() * 1000)\n",
    "data = [\n",
    "    (current_time - 7000, \"sensor_1\", 23.5),\n",
    "    (current_time - 6000, \"sensor_2\", 24.1),\n",
    "    (current_time - 5000, \"sensor_1\", 25.3),\n",
    "    (current_time - 4000, \"sensor_3\", 22.5),\n",
    "    (current_time - 3000, \"sensor_2\", 24.6),\n",
    "    (current_time - 2000, \"sensor_3\", 21.7),\n",
    "    (current_time - 1000, \"sensor_1\", 25.0),\n",
    "]\n",
    "\n",
    "# Create a data stream from the sample data\n",
    "data_stream = env.from_collection(\n",
    "    collection=data,\n",
    "    type_info=Types.TUPLE([Types.LONG(), Types.STRING(), Types.FLOAT()])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "watermark_strategy = WatermarkStrategy.for_bounded_out_of_orderness(Duration.of_seconds(5))\n",
    "timestamped_stream = data_stream.assign_timestamps_and_watermarks(watermark_strategy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyflink.table import Schema\n",
    "\n",
    "# Convert DataStream to Table with Schema\n",
    "table = t_env.from_data_stream(\n",
    "    timestamped_stream  ,\n",
    "    Schema.new_builder()\n",
    "        .column_by_expression(\"data_timestamp\", \"f0\")\n",
    "        .column_by_expression(\"sensor_id\", \"f1\")\n",
    "        .column_by_expression(\"temperature\", \"f2\")\n",
    "        .build()\n",
    ")\n",
    "\n",
    "# Correct way to create a temporary view in the Table Environment\n",
    "t_env.create_temporary_view(\"sensor_data\", table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------------+--------------------------------+--------------------------------+\n",
      "| op |       data_timestamp |                      sensor_id |                    temperature |\n",
      "+----+----------------------+--------------------------------+--------------------------------+\n",
      "| +I |        1734482513866 |                       sensor_1 |                           23.5 |\n",
      "| +I |        1734482514866 |                       sensor_2 |                           24.1 |\n",
      "| +I |        1734482515866 |                       sensor_1 |                           25.3 |\n",
      "| +I |        1734482516866 |                       sensor_3 |                           22.5 |\n",
      "| +I |        1734482517866 |                       sensor_2 |                           24.6 |\n",
      "| +I |        1734482518866 |                       sensor_3 |                           21.7 |\n",
      "| +I |        1734482519866 |                       sensor_1 |                           25.0 |\n",
      "+----+----------------------+--------------------------------+--------------------------------+\n",
      "7 rows in set\n"
     ]
    }
   ],
   "source": [
    "df = t_env.execute_sql(\"select data_timestamp,sensor_id,temperature from sensor_data\")\n",
    "df.print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudDetectionFunction(ProcessFunction):\n",
    "    def process_element(self, element, ctx):\n",
    "        transaction_id, account_id, amount, timestamp = element\n",
    "\n",
    "        # Check if the transaction amount exceeds $10,000\n",
    "        if amount > 10000:\n",
    "            alert_msg = f\"ALERT: Suspicious transaction detected - Transaction ID: {transaction_id}, Account: {account_id}, Amount: ${amount}\"\n",
    "            return [alert_msg]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StreamExecutionEnvironment.get_execution_environment()\n",
    "env.set_parallelism(1)\n",
    "\n",
    "# Sample transaction data (transaction_id, account_id, amount, timestamp)\n",
    "transaction_data = [\n",
    "    (\"txn1\", \"acc101\", 5000, 1653232900),\n",
    "    (\"txn2\", \"acc102\", 15000, 1653232910),\n",
    "    (\"txn3\", \"acc103\", 12000, 1653232920),\n",
    "    (\"txn4\", \"acc104\", 9000, 1653232930),\n",
    "    (\"txn5\", \"acc105\", 11000, 1653232940)\n",
    "]\n",
    "\n",
    "# Create a DataStream from the sample data\n",
    "transaction_stream = env.from_collection(\n",
    "    collection=transaction_data,\n",
    "    type_info=Types.TUPLE([Types.STRING(), Types.STRING(), Types.INT(), Types.LONG()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALERT: Suspicious transaction detected - Transaction ID: txn2, Account: acc102, Amount: $15000\n",
      "ALERT: Suspicious transaction detected - Transaction ID: txn3, Account: acc103, Amount: $12000\n",
      "ALERT: Suspicious transaction detected - Transaction ID: txn5, Account: acc105, Amount: $11000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyflink.common.job_execution_result.JobExecutionResult at 0x1481b8ad0>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suspicious_transactions = transaction_stream.process(FraudDetectionFunction())\n",
    "\n",
    "# Print the detected alerts\n",
    "suspicious_transactions.print()\n",
    "\n",
    "# Execute the Flink job\n",
    "env.execute(\"Fraud Detection in Financial Transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
