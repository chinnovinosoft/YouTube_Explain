{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A \"token\" is typically a word, part of a word, or even a single character, depending on the encoding used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every word, punctuation mark, and sometimes even part of a word, is split into tokens. These tokens are what the model processes, and each model has a limit on how many tokens it can handle in one request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'multiply',\n",
       "  'args': {'a': 119, 'b': 8},\n",
       "  'id': 'call_xgNN2zPIgY9iFhj8vI2uKlWM',\n",
       "  'type': 'tool_call'},\n",
       " {'name': 'add',\n",
       "  'args': {'a': -20, 'b': 0},\n",
       "  'id': 'call_NLoPj1PUUFqctwuJ65KyVqDa',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies a and b.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "tools = [add, multiply]\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "examples = [\n",
    "    HumanMessage(\n",
    "        \"What's the product of 317253 and 128472 plus four\", name=\"example_user\"\n",
    "    ),\n",
    "    AIMessage(\n",
    "        \"\",\n",
    "        name=\"example_assistant\",\n",
    "        tool_calls=[\n",
    "            {\"name\": \"Multiply\", \"args\": {\"x\": 317253, \"y\": 128472}, \"id\": \"1\"}\n",
    "        ],\n",
    "    ),\n",
    "    ToolMessage(\"16505054784\", tool_call_id=\"1\"),\n",
    "    AIMessage(\n",
    "        \"\",\n",
    "        name=\"example_assistant\",\n",
    "        tool_calls=[{\"name\": \"Add\", \"args\": {\"x\": 16505054784, \"y\": 4}, \"id\": \"2\"}],\n",
    "    ),\n",
    "    ToolMessage(\"16505054788\", tool_call_id=\"2\"),\n",
    "    AIMessage(\n",
    "        \"The product of 317253 and 128472 plus four is 16505054788\",\n",
    "        name=\"example_assistant\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "system = \"\"\"You are bad at math but are an expert at using a calculator. \n",
    "\n",
    "Use past tool usage as an example of how to correctly use the tools.\"\"\"\n",
    "few_shot_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        *examples,\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = {\"query\": RunnablePassthrough()} | few_shot_prompt | llm_with_tools\n",
    "chain.invoke(\"Whats 119 times 8 minus 20\").tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    "    trim_messages,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\"you're a good assistant, you always respond with a joke.\"),\n",
    "    HumanMessage(\"i wonder why it's called langchain\"),\n",
    "    AIMessage(\n",
    "        'Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!'\n",
    "    ),\n",
    "    HumanMessage(\"and who is harrison chasing anyways\"),\n",
    "    AIMessage(\n",
    "        \"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\"\n",
    "    ),\n",
    "    HumanMessage(\"what do you call a speechless parrot\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trim_messages(\n",
    "    messages, #This is the list of conversation messages that you want to trim.\n",
    "    strategy=\"last\", #This tells the function how to trim the messages. it will keep the last n_count tokens. So, it trims the conversation by keeping the most recent part\n",
    "    token_counter=ChatOpenAI(model=\"gpt-4o\"),#This is the tool used to count how many tokens (pieces of language) are in the conversation. The function needs to know how many tokens your messages have, so it uses a model like gpt-4o to count the tokens\n",
    "    max_tokens=45,#This sets the maximum number of tokens (words or characters) that can be in the trimmed conversation\n",
    "    start_on=\"human\",#This specifies where the trimmed conversation should start. In this case, it tells the function to start with a message from a human\n",
    "    end_on=(\"human\", \"tool\"),#This specifies where the trimmed conversation should end. The function will stop trimming when it reaches a message from a human or a tool\n",
    "    include_system=True, #This tells the function to keep the SystemMessage in the conversation if it exists. \n",
    "    allow_partial=True, #When allow_partial is set to False, the function will only keep entire messages, not fragments. So, if a message doesn't fully fit in the token limit, it will be cut entirely rather than keeping part of it.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='and who is harrison chasing anyways', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trim_messages(\n",
    "    messages, #This is the list of conversation messages that you want to trim.\n",
    "    strategy=\"last\", #This tells the function how to trim the messages. it will keep the last n_count tokens. So, it trims the conversation by keeping the most recent part\n",
    "    token_counter=ChatOpenAI(model=\"gpt-4o\"),#This is the tool used to count how many tokens (pieces of language) are in the conversation. The function needs to know how many tokens your messages have, so it uses a model like gpt-4o to count the tokens\n",
    "    max_tokens=75,#This sets the maximum number of tokens (words or characters) that can be in the trimmed conversation\n",
    "    start_on=\"human\",#This specifies where the trimmed conversation should start. In this case, it tells the function to start with a message from a human\n",
    "    end_on=(\"human\", \"tool\"),#This specifies where the trimmed conversation should end. The function will stop trimming when it reaches a message from a human or a tool\n",
    "    include_system=True, #This tells the function to keep the SystemMessage in the conversation if it exists. \n",
    "    allow_partial=True, #When allow_partial is set to False, the function will only keep entire messages, not fragments. So, if a message doesn't fully fit in the token limit, it will be cut entirely rather than keeping part of it.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='and who is harrison chasing anyways', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trim_messages(\n",
    "    messages, #This is the list of conversation messages that you want to trim.\n",
    "    # strategy=\"last\", #This tells the function how to trim the messages. it will keep the last n_count tokens. So, it trims the conversation by keeping the most recent part\n",
    "    token_counter=ChatOpenAI(model=\"gpt-4o\"),#This is the tool used to count how many tokens (pieces of language) are in the conversation. The function needs to know how many tokens your messages have, so it uses a model like gpt-4o to count the tokens\n",
    "    max_tokens=75,#This sets the maximum number of tokens (words or characters) that can be in the trimmed conversation\n",
    "    start_on=\"human\",#This specifies where the trimmed conversation should start. In this case, it tells the function to start with a message from a human\n",
    "    end_on=(\"human\", \"tool\"),#This specifies where the trimmed conversation should end. The function will stop trimming when it reaches a message from a human or a tool\n",
    "    include_system=True, #This tells the function to keep the SystemMessage in the conversation if it exists. \n",
    "    allow_partial=True, #When allow_partial is set to False, the function will only keep entire messages, not fragments. So, if a message doesn't fully fit in the token limit, it will be cut entirely rather than keeping part of it.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 83,\n",
       "  'prompt_tokens': 32,\n",
       "  'total_tokens': 115,\n",
       "  'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0},\n",
       "  'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}},\n",
       " 'model_name': 'gpt-4o-2024-08-06',\n",
       " 'system_fingerprint': 'fp_a20a4ee344',\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "llm.invoke(\"\"\"Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!\"\"\").response_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 79,\n",
       "  'prompt_tokens': 33,\n",
       "  'total_tokens': 112,\n",
       "  'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0},\n",
       "  'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}},\n",
       " 'model_name': 'gpt-4o-2024-08-06',\n",
       " 'system_fingerprint': 'fp_a7d06e42a7',\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "llm.invoke(\"\"\"Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it! man\"\"\").response_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 92,\n",
       "  'prompt_tokens': 34,\n",
       "  'total_tokens': 126,\n",
       "  'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0},\n",
       "  'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}},\n",
       " 'model_name': 'gpt-4o-2024-08-06',\n",
       " 'system_fingerprint': 'fp_a7d06e42a7',\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "llm.invoke(\"\"\"Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it! man \"\"\").response_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 92,\n",
       "  'prompt_tokens': 37,\n",
       "  'total_tokens': 129,\n",
       "  'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0},\n",
       "  'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}},\n",
       " 'model_name': 'gpt-4o-2024-08-06',\n",
       " 'system_fingerprint': 'fp_a7d06e42a7',\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "llm.invoke(\"\"\"Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it! man \"women\" \"\"\").response_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\"you're a good assistant, you always respond with a joke.\"),\n",
    "    HumanMessage(\"i wonder why it's called langchain\"),\n",
    "    AIMessage(\n",
    "        'Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!'\n",
    "    ),\n",
    "    HumanMessage(\"and who is harrison chasing anyways\"),\n",
    "    AIMessage(\n",
    "        \"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office! and some times he also chages coffe even in the house as he is a coffe lover and might be he wanna be born as a cofee bean in the next birth, incase if he has got next birth byy god \"\n",
    "    ),\n",
    "    HumanMessage(\"what do you call a speechless parrot\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trim_messages(\n",
    "    messages, #This is the list of conversation messages that you want to trim.\n",
    "    strategy=\"last\", #This tells the function how to trim the messages. it will keep the last n_count tokens. So, it trims the conversation by keeping the most recent part\n",
    "    token_counter=ChatOpenAI(model=\"gpt-4o\"),#This is the tool used to count how many tokens (pieces of language) are in the conversation. The function needs to know how many tokens your messages have, so it uses a model like gpt-4o to count the tokens\n",
    "    max_tokens=75,#This sets the maximum number of tokens (words or characters) that can be in the trimmed conversation\n",
    "    start_on=\"human\",#This specifies where the trimmed conversation should start. In this case, it tells the function to start with a message from a human\n",
    "    end_on=(\"human\", \"tool\"),#This specifies where the trimmed conversation should end. The function will stop trimming when it reaches a message from a human or a tool\n",
    "    include_system=True, #This tells the function to keep the SystemMessage in the conversation if it exists. \n",
    "    allow_partial=True, #When allow_partial is set to False, the function will only keep entire messages, not fragments. So, if a message doesn't fully fit in the token limit, it will be cut entirely rather than keeping part of it.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='and who is harrison chasing anyways', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office! and some times he also chages coffe even in the house as he is a coffe lover and might be he wanna be born as a cofee bean in the next birth, incase if he has got next birth byy god \", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trim_messages(\n",
    "    messages, #This is the list of conversation messages that you want to trim.\n",
    "    strategy=\"last\", #This tells the function how to trim the messages. it will keep the last n_count tokens. So, it trims the conversation by keeping the most recent part\n",
    "    token_counter=ChatOpenAI(model=\"gpt-4o\"),#This is the tool used to count how many tokens (pieces of language) are in the conversation. The function needs to know how many tokens your messages have, so it uses a model like gpt-4o to count the tokens\n",
    "    max_tokens=120,#This sets the maximum number of tokens (words or characters) that can be in the trimmed conversation\n",
    "    start_on=\"human\",#This specifies where the trimmed conversation should start. In this case, it tells the function to start with a message from a human\n",
    "    end_on=(\"human\", \"tool\"),#This specifies where the trimmed conversation should end. The function will stop trimming when it reaches a message from a human or a tool\n",
    "    include_system=True, #This tells the function to keep the SystemMessage in the conversation if it exists. \n",
    "    allow_partial=True, #When allow_partial is set to False, the function will only keep entire messages, not fragments. So, if a message doesn't fully fit in the token limit, it will be cut entirely rather than keeping part of it.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trim_messages(\n",
    "    messages,\n",
    "    strategy=\"last\",\n",
    "    token_counter=len,\n",
    "    max_tokens=2,\n",
    "    start_on=\"human\",\n",
    "    end_on=(\"human\", \"tool\"),\n",
    "    include_system=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"i wonder why it's called langchain\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='and who is harrison chasing anyways', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office! and some times he also chages coffe even in the house as he is a coffe lover and might be he wanna be born as a cofee bean in the next birth, incase if he has got next birth byy god \", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trim_messages(\n",
    "    messages,\n",
    "    strategy=\"last\",\n",
    "    token_counter=len,\n",
    "    max_tokens=6,\n",
    "    start_on=\"human\",\n",
    "    end_on=(\"human\", \"tool\"),\n",
    "    include_system=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trim_messages(\n",
    "    messages,\n",
    "    max_tokens=75,\n",
    "    strategy=\"last\",\n",
    "    token_counter=ChatOpenAI(model=\"gpt-4o\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Custom Token Counter !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "import tiktoken\n",
    "from langchain_core.messages import BaseMessage, ToolMessage\n",
    "\n",
    "\n",
    "def str_token_counter(text: str) -> int:\n",
    "    enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "\n",
    "def tiktoken_counter(messages: List[BaseMessage]) -> int:\n",
    "    num_tokens =0\n",
    "    for msg in messages:\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            role = \"user\"\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            role = \"assistant\"\n",
    "        elif isinstance(msg, ToolMessage):\n",
    "            role = \"tool\"\n",
    "        elif isinstance(msg, SystemMessage):\n",
    "            role = \"system\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported messages type {msg.__class__}\")\n",
    "        num_tokens += (\n",
    "            + str_token_counter(role)\n",
    "            + str_token_counter(msg.content)\n",
    "        )\n",
    "        if msg.name:\n",
    "            num_tokens += str_token_counter(msg.name)\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "trim_messages(\n",
    "    messages,\n",
    "    token_counter=tiktoken_counter,\n",
    "    strategy=\"last\",\n",
    "    max_tokens=45,\n",
    "    start_on=\"human\",\n",
    "    end_on=(\"human\", \"tool\"),\n",
    "    include_system=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://api.python.langchain.com/en/latest/messages/langchain_core.messages.ai.AIMessage.html\n",
    "\n",
    "https://api.python.langchain.com/en/latest/messages/langchain_core.messages.human.HumanMessage.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import filter_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\"you are a good assistant who can answer funny on sports\", id=\"1\"),\n",
    "    HumanMessage(\"Why do people get addicted to badminton\", id=\"2\", name=\"praveen\"),\n",
    "    AIMessage(\"People when they get to enjoy the game then fall in love with that sport, ideally in the case of badminton, they get smash so, thye love badminton\", id=\"3\", name=\"chatgpt\"),\n",
    "    HumanMessage(\"you sound funny but, you are correct\", id=\"4\", name=\"praveen\"),\n",
    "    AIMessage(\"thanks for the appretiation\", id=\"5\", name=\"chatgpt\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Why do people get addicted to badminton', additional_kwargs={}, response_metadata={}, name='praveen', id='2'),\n",
       " HumanMessage(content='you sound funny but, you are correct', additional_kwargs={}, response_metadata={}, name='praveen', id='4')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_messages(messages, include_types=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='you are a good assistant who can answer funny on sports', additional_kwargs={}, response_metadata={}, id='1'),\n",
       " AIMessage(content='People when they get to enjoy the game then fall in love with that sport, ideally in the case of badminton, they get smash so, thye love badminton', additional_kwargs={}, response_metadata={}, name='chatgpt', id='3'),\n",
       " AIMessage(content='thanks for the appretiation', additional_kwargs={}, response_metadata={}, name='chatgpt', id='5')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_messages(messages, exclude_names=[\"praveen\", \"chinna\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You're welcome! If you have any more questions or need a good laugh about sports, just let me know! Why did the football team go to the bank? Because they wanted to get their quarterback! 🏈😄\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 45, 'prompt_tokens': 72, 'total_tokens': 117, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_7693ae462b', 'finish_reason': 'stop', 'logprobs': None}, id='run-fd01d332-bc54-4392-a6bd-688830606ab0-0', usage_metadata={'input_tokens': 72, 'output_tokens': 45, 'total_tokens': 117, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_ = filter_messages(exclude_names=[\"praveen\", \"chinna\"])\n",
    "chain = filter_ | llm\n",
    "chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import merge_message_runs\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"you're a good assistant with lot of humour.\"),\n",
    "    SystemMessage(\"you always respond very funny.\"),\n",
    "    HumanMessage(\"i wonder why people like sports\"),\n",
    "    HumanMessage(\"and why is badminton very addictive\")\n",
    "]\n",
    "\n",
    "merged = merge_message_runs(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant with lot of humour.\\nyou always respond very funny.\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='i wonder why people like sports\\nand why is badminton very addictive', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ah, the age-old question: why do people like sports? It’s simple! Sports are like reality TV, but with fewer dramatic confessions and more sweat. People love the thrill, the competition, and the chance to yell at the TV as if the players can hear them. Plus, where else can you wear stretchy pants in public and call it “athletic wear”?\\n\\nAs for badminton being addictive, it’s probably because it’s the only sport where you can feel like a ninja while wielding a racket! You get to smash things (the shuttlecock, not your neighbor’s window), and it’s a great excuse to run around like a kid again. Plus, who doesn’t love a sport where you can say “I’m just going to play a quick game of badminton” and then end up playing for hours, completely forgetting about adult responsibilities? It’s like a mini-vacation from reality, with a side of cardio! 🏸😄', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 195, 'prompt_tokens': 39, 'total_tokens': 234, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_482c22a7bc', 'finish_reason': 'stop', 'logprobs': None}, id='run-c66a791d-cfd8-4b52-b297-9749dd1977f1-0', usage_metadata={'input_tokens': 39, 'output_tokens': 195, 'total_tokens': 234, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(merged)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
