{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#RunnableParallel is a class designed to run multiple tasks or chains in parallel. This means it can execute several functions, prompts, or actions at the same time rather than one after the other, which can significantly speed up workflows that don’t rely on each other’s results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': AIMessage(content='Why do married people live longer?\\n\\nBecause they can’t argue with their spouse if they’re dead!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 13, 'total_tokens': 33, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e2bde53e6e', 'finish_reason': 'stop', 'logprobs': None}, id='run-01b591ba-d562-454d-82de-5ff5d2814163-0', usage_metadata={'input_tokens': 13, 'output_tokens': 20, 'total_tokens': 33, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}),\n",
       " 'poem': AIMessage(content='Two hearts entwined, in laughter and tears,  \\nA journey of love that conquers all fears.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 15, 'total_tokens': 36, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e2bde53e6e', 'finish_reason': 'stop', 'logprobs': None}, id='run-c9cf804b-6b46-42e6-b729-c961bc63604a-0', usage_metadata={'input_tokens': 15, 'output_tokens': 21, 'total_tokens': 36, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "joke_chain = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model\n",
    "poem_chain = (\n",
    "    ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\") | model\n",
    ")\n",
    "\n",
    "map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)\n",
    "\n",
    "map_chain.invoke({\"topic\": \"marriage\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#RunnablePasthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Praveen's full name is Praveen Reddy C.\""
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "retrieval_chain = (\n",
    "    {\"context\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "retrieval_chain.invoke({\"question\":\"what is praveen full name?\",\"context\":\"Praveen full name is Praveen Reddy C and he is from Munagala Village, He loves his village, His father is farmer who cultivates fruits and palm oil\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Praveen's full name is Praveen Reddy C and his village name is Munagala Village.\""
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "retrieval_chain.invoke({\"question\":\"what is praveen full name and his village name?\",\"context\":\"Praveen full name is Praveen Reddy C and he is from Munagala Village, He loves his village, His father is farmer who cultivates fruits and palm oil\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EQUATION: x^3 + 7 = 12\n",
      "\n",
      "SOLUTION: \n",
      "Subtract 7 from both sides:\n",
      "x^3 = 5\n",
      "\n",
      "Take the cube root of both sides:\n",
      "x = ∛5\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages( \n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Write out the following equation using algebraic symbols then solve it. Use the format\\n\\nEQUATION:...\\nSOLUTION:...\\n\\n\",\n",
    "        ),\n",
    "        (\"human\", \"{equation_statement}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "\n",
    "runnable = (\n",
    "    {\"equation_statement\": RunnablePassthrough()} | prompt | model | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(runnable.invoke(\"x raised to the third plus seven equals 12\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#When you use .bind(stop=\"SOLUTION\"), you’re telling the model that, every time it generates output, it should stop when it reaches the word \"SOLUTION\"\n",
    "\n",
    "#In short, .bind(stop=\"SOLUTION\") ensures that the OpenAI model knows when to stop generating text (after the solution is reached), making the output more predictable and structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EQUATION: x^3 + 7 = 12\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "runnable = (\n",
    "    {\"equation_statement\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model.bind(stop=\"SOLUTION\")\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(runnable.invoke(\"x raised to the third plus seven equals 12\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Custom Function \n",
    "\n",
    "#in which we are passing argument from one step to next step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My dearest Sahasra, you fill my heart with joy and love every single day.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "def length_function(d:dict):\n",
    "    length  = 0\n",
    "    for k,v in d.items():\n",
    "        length = length + len(v)\n",
    "    return length\n",
    "\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Writing a Love Letter by a {a} to {b} with {wrd} words\")\n",
    "\n",
    "chain1 = prompt | model\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"a\": itemgetter(\"name1\"),   # Extract name1 from input dict\n",
    "        \"b\": itemgetter(\"name2\"),   # Extract name2 from input dict\n",
    "        \"wrd\": RunnableLambda(lambda x: length_function({\"name1\": x[\"name1\"], \"name2\": x[\"name2\"]}))  \n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"name1\": \"Praveen\", \"name2\": \"Sahasra\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Create a chain with a python method !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The subject of this joke is scarecrows and successful marriages.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\"What is the subject of this joke: {joke}\")\n",
    "\n",
    "\n",
    "@chain\n",
    "def custom_chain(text):\n",
    "    prompt_val1 = prompt1.invoke({\"topic\": text})\n",
    "    output1 = ChatOpenAI().invoke(prompt_val1)\n",
    "    parsed_output1 = StrOutputParser().invoke(output1)\n",
    "    chain2 = prompt2 | ChatOpenAI() | StrOutputParser()\n",
    "    return chain2.invoke({\"joke\": parsed_output1})\n",
    "\n",
    "\n",
    "custom_chain.invoke(\"marriage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#runtime chain internals\n",
    "\n",
    "##configurable_fields method\n",
    "\n",
    "This allows you to adjust certain settings (like temperature or parameters) for a specific task or step in a process, while the process is running.\n",
    "\n",
    "Think of it as giving you control over individual steps in a chain, without needing to decide everything in advance (like with .bind()).\n",
    "\n",
    "Example: \n",
    "If you're generating text using an AI model, you can change how \"creative\" (temperature) the AI is during runtime, rather than locking in this value before the process starts.\n",
    "\n",
    "-------------\n",
    "\n",
    "##configurable_alternatives method:\n",
    "\n",
    "This allows you to swap out different methods or models during runtime.\n",
    "\n",
    "You can list multiple alternative options (for example, different AI models or ways of solving a problem) and choose which one to use when the process runs.\n",
    "\n",
    "Example: \n",
    "If you have two AI models (like GPT-3 and another model), you can switch between them as needed without changing your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='My dearest love, you are my everything, my heart beats only for you. I love you.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 15, 'total_tokens': 36, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-35ea896e-37a0-40aa-a93b-01c097f2c36a-0', usage_metadata={'input_tokens': 15, 'output_tokens': 21, 'total_tokens': 36, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(temperature=0).configurable_fields(\n",
    "    temperature=ConfigurableField(\n",
    "        id=\"llm_temperature\",\n",
    "        name=\"LLM Temperature\",\n",
    "        description=\"The temperature of the LLM\",\n",
    "    )\n",
    ")\n",
    "\n",
    "model.invoke(\"Write a Love Letter with 15 words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='My darling, you are my light, my heart, my everything. I love you endlessly and forever.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 15, 'total_tokens': 36, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-54c78160-c797-497a-a6af-b10842d07bc2-0', usage_metadata={'input_tokens': 15, 'output_tokens': 21, 'total_tokens': 36, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.with_config(configurable={\"llm_temperature\": 0.9}).invoke(\"Write a Love Letter with 15 words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='235', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 14, 'total_tokens': 15, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-6a382466-c544-4bad-aa59-14d5ec3db4ee-0', usage_metadata={'input_tokens': 14, 'output_tokens': 1, 'total_tokens': 15, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"Pick a random number above {x}\")\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\"x\": 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-3-haiku-20240307\", temperature=0\n",
    ").configurable_alternatives(\n",
    "    ConfigurableField(id=\"llm\"),\n",
    "    default_key=\"anthropic\",\n",
    "    openai=ChatOpenAI(),\n",
    "    gpt4=ChatOpenAI(model=\"gpt-4\"),\n",
    ")\n",
    "prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "chain = prompt | llm\n",
    "\n",
    "# By default it will call Anthropic\n",
    "chain..with_config(configurable={\"id\": \"llm\"})invoke({\"topic\": \"Love\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Write A 3 liner love story on my Wife {name}\"\n",
    ").configurable_alternatives(\n",
    "    ConfigurableField(id=\"prompt\"),\n",
    "    default_key=\"love\",\n",
    "    poem=PromptTemplate.from_template(\"Write A 3 line love poem to my Love {name}\"),\n",
    ")\n",
    "chain = prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a bustling café, Sahasra's laughter danced through the air, capturing my heart like a melody. With every shared glance, our souls intertwined, painting a love story only we could understand. Together, we built a world where every moment felt like a beautiful forever.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({\"name\": \"Sahasra\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the garden of dreams, your laughter blooms bright,  \n",
      "Naveena, my heart dances in your gentle light,  \n",
      "Together we weave a tapestry of love, pure and right.\n"
     ]
    }
   ],
   "source": [
    "print(chain.with_config(configurable={\"prompt\": \"poem\"}).invoke({\"name\": \"Naveena\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Glue'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "prompt = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"Given the user question below, classify it as either being about `Glue`, `Databricks`, or `Other`.\n",
    "\n",
    "Do not respond with more than one word.\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Classification:\"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "chain.invoke({\"question\": \"how do I cuse AWS Glue ?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_chain = PromptTemplate.from_template(\n",
    "    \"\"\"You are an expert in AWS ETL Service Glue. \\\n",
    "Always answer questions starting with \"As the AWS Certified Praveen Reddy told me\". \\\n",
    "Respond to the following question in 2 lines:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    ") | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "\n",
    "databricks_chain = PromptTemplate.from_template(\n",
    "    \"\"\"You are an expert in Databricks. \\\n",
    "Always answer questions starting with \"As Legendary Databricks Expert Chinnareddy once told \". \\\n",
    "Respond to the following question in 2 lines: \n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    ") | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "\n",
    "general_chain = PromptTemplate.from_template(\n",
    "    \"\"\"Respond to the following question in 2 lines:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    ") | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route(info):\n",
    "    if \"glue\" in info[\"topic\"].lower():\n",
    "        return glue_chain\n",
    "    elif \"databricks\" in info[\"topic\"].lower():\n",
    "        return databricks_chain\n",
    "    else:\n",
    "        return general_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "full_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | RunnableLambda(\n",
    "    route\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Glue'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"How to Write a Glue Catalog in Glue?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='As the AWS Certified Praveen Reddy told me, you can write a Glue Catalog in Glue by using the AWS Glue Console or the AWS Glue API to create and manage tables and databases. Additionally, you can use Glue Crawlers to automatically discover and catalog your data sources.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 56, 'prompt_tokens': 59, 'total_tokens': 115, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e2bde53e6e', 'finish_reason': 'stop', 'logprobs': None}, id='run-3a42de12-7340-4526-ba02-f1a84edd5078-0', usage_metadata={'input_tokens': 59, 'output_tokens': 56, 'total_tokens': 115, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"How to Write a Glue Catalog in Glue?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Databricks'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"What is Unity Catalog in Databricks\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='As Legendary Databricks Expert Chinnareddy once told, Unity Catalog is a unified governance solution for all data assets in Databricks, enabling fine-grained access control and data discovery. It simplifies data management across various workspaces and provides a centralized view of data assets.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 56, 'prompt_tokens': 59, 'total_tokens': 115, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e2bde53e6e', 'finish_reason': 'stop', 'logprobs': None}, id='run-4b973a80-b306-4e7e-a323-dfb22a3e01d3-0', usage_metadata={'input_tokens': 59, 'output_tokens': 56, 'total_tokens': 115, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"What is Unity Catalog in Databricks\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Inspect Runnables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    +-------------------------------+                      \n",
      "                    | Parallel<topic,question>Input |                      \n",
      "                    +-------------------------------+                      \n",
      "                             ***           ***                             \n",
      "                           **                 **                           \n",
      "                         **                     **                         \n",
      "              +----------------+                  **                       \n",
      "              | PromptTemplate |                   *                       \n",
      "              +----------------+                   *                       \n",
      "                       *                           *                       \n",
      "                       *                           *                       \n",
      "                       *                           *                       \n",
      "                +------------+                     *                       \n",
      "                | ChatOpenAI |                     *                       \n",
      "                +------------+                     *                       \n",
      "                       *                           *                       \n",
      "                       *                           *                       \n",
      "                       *                           *                       \n",
      "              +-----------------+             +--------+                   \n",
      "              | StrOutputParser |             | Lambda |                   \n",
      "              +-----------------+             +--------+                   \n",
      "                             ***           ***                             \n",
      "                                **       **                                \n",
      "                                  **   **                                  \n",
      "                    +--------------------------------+                     \n",
      "                    | Parallel<topic,question>Output |                     \n",
      "                    +--------------------------------+                     \n",
      "                                     *                                     \n",
      "                                     *                                     \n",
      "                                     *                                     \n",
      "                              +-------------+                              \n",
      "                              | route_input |                              \n",
      "                          ****+-------------+****                          \n",
      "                     *****           *           *****                     \n",
      "                 ****                *                ****                 \n",
      "              ***                    *                    ***              \n",
      "+----------------+          +----------------+          +----------------+ \n",
      "| PromptTemplate |          | PromptTemplate |          | PromptTemplate | \n",
      "+----------------+          +----------------+          +----------------+ \n",
      "         *                           *                           *         \n",
      "         *                           *                           *         \n",
      "         *                           *                           *         \n",
      "  +------------+              +------------+              +------------+   \n",
      "  | ChatOpenAI |*             | ChatOpenAI |              | ChatOpenAI |   \n",
      "  +------------+ ****         +------------+          ****+------------+   \n",
      "                     *****           *           *****                     \n",
      "                          ****       *       ****                          \n",
      "                              ***    *    ***                              \n",
      "                             +--------------+                              \n",
      "                             | route_output |                              \n",
      "                             +--------------+                              \n"
     ]
    }
   ],
   "source": [
    "full_chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
