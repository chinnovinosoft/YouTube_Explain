{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Observability:  refers to the ability to monitor, debug, and understand your LLM (Large Language Model) applications at every stepâ€”just like how you'd use logging, metrics, and traces for regular software systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"youtube-trail-1\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"\"\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] =\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "content =\"\"\"Praveen Reddy is a passionate software engineer with a strong enthusiasm for cutting-edge technology. \n",
    "            He thrives in fast-paced startup \n",
    "            environments where innovation and agility are key. Known for his curiosity and dedication, \n",
    "            Praveen enjoys building impactful solutions that make a difference.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "\n",
    "def retriever(query: str):\n",
    "    results = [f\"{content}\"]\n",
    "    return results\n",
    "\n",
    "def rag(question):\n",
    "    docs = retriever(question)\n",
    "    system_message = \"\"\"Answer the users question using only the provided information below:\n",
    "    \n",
    "    {docs}\"\"\".format(docs=\"\\n\".join(docs))\n",
    "    \n",
    "    return openai_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "        model=\"gpt-4o-mini\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-BT6AR04hGBqIoMIDiD7WE7X2HxLev', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Praveen Reddy is a dedicated software engineer passionate about creating impactful solutions in fast-paced startup environments.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746274919, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_0392822090', usage=CompletionUsage(completion_tokens=22, prompt_tokens=90, total_tokens=112, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(\"give me 1 liner about praveen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from langsmith import traceable\n",
    "\n",
    "# For fully automatic tracing of every OpenAI API call, wrap the client once and keep using it.\n",
    "\n",
    "openai_client = wrap_openai(OpenAI())\n",
    "\n",
    "\n",
    "def retriever(query: str):\n",
    "    results = [f\"{content}\"]\n",
    "    return results\n",
    "\n",
    "def rag(question):\n",
    "    docs = retriever(question)\n",
    "    system_message = \"\"\"Answer the users question using only the provided information below:\n",
    "    \n",
    "    {docs}\"\"\".format(docs=\"\\n\".join(docs))\n",
    "    \n",
    "    return openai_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "        model=\"gpt-4o-mini\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-BT6B9zuqNWeDmqRDRYPpaF5NMauHS', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Praveen Reddy is a dedicated software engineer who thrives in fast-paced startup environments, leveraging cutting-edge technology to build impactful solutions.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746274963, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_0392822090', usage=CompletionUsage(completion_tokens=28, prompt_tokens=90, total_tokens=118, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(\"give me 1 liner about praveen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from langsmith import traceable\n",
    "\n",
    "openai_client = wrap_openai(OpenAI())\n",
    "\n",
    "\n",
    "def retriever(query: str):\n",
    "    results = [f\"{content}\"]\n",
    "    return results\n",
    "\n",
    "@traceable\n",
    "def rag(question):\n",
    "    docs = retriever(question)\n",
    "    system_message = \"\"\"Answer the users question using only the provided information below:\n",
    "    \n",
    "    {docs}\"\"\".format(docs=\"\\n\".join(docs))\n",
    "    \n",
    "    return openai_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "        model=\"gpt-4o-mini\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-BT6DOWu9wHkptLU19z74BvALOcA2f', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Praveen Reddy is a dedicated software engineer who thrives in dynamic startup environments, driven by a passion for innovative technology and impactful solutions.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746275102, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_0392822090', usage=CompletionUsage(completion_tokens=29, prompt_tokens=90, total_tokens=119, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(\"give me 1 liner about praveen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from langsmith import traceable\n",
    "openai_client = wrap_openai(OpenAI())\n",
    "\n",
    "\n",
    "@traceable(run_type=\"retriever\")\n",
    "def retriever(query: str):\n",
    "    results = [f\"{content}\"]\n",
    "    return results\n",
    "\n",
    "@traceable\n",
    "def rag(question):\n",
    "    docs = retriever(question)\n",
    "    system_message = \"\"\"Answer the users question using only the provided information below:\n",
    "    \n",
    "    {docs}\"\"\".format(docs=\"\\n\".join(docs))\n",
    "    \n",
    "    return openai_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "        model=\"gpt-4o-mini\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag(\"give me 1 liner about praveen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-BT6FXqFfsN0hBRJwdjLdR9Mkk2aQe', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Praveen Reddy is a dedicated software engineer who excels in fast-paced startups, driven by a passion for innovation and impactful technology solutions.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746275235, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_0392822090', usage=CompletionUsage(completion_tokens=29, prompt_tokens=90, total_tokens=119, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "run_id = str(uuid.uuid4())\n",
    "rag(\n",
    "    \"give me 1 liner about praveen\",\n",
    "    langsmith_extra={\"run_id\": run_id}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c85b1b18-5c2c-4822-883f-54bc64186035'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feedback(id=UUID('d03036b1-2a66-4cf8-9c70-b1b216dc8277'), created_at=datetime.datetime(2025, 5, 3, 12, 29, 24, 543651, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2025, 5, 3, 12, 29, 24, 543654, tzinfo=datetime.timezone.utc), run_id=UUID('c85b1b18-5c2c-4822-883f-54bc64186035'), trace_id=None, key='user-score', score=1.0, value=None, comment=None, correction=None, feedback_source=FeedbackSourceBase(type='api', metadata={}, user_id=None, user_name=None), session_id=None, comparative_experiment_id=None, feedback_group_id=None, extra=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "ls_client = Client()\n",
    "\n",
    "ls_client.create_feedback(\n",
    "    run_id,\n",
    "    key=\"user-score\",\n",
    "    score=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from langsmith import traceable\n",
    "openai_client = wrap_openai(OpenAI())\n",
    "\n",
    "\n",
    "@traceable(run_type=\"retriever\")\n",
    "def retriever(query: str):\n",
    "    results = [f\"{content}\"]\n",
    "    return results\n",
    "\n",
    "@traceable(metadata={\"llm\": \"gpt-4o-mini\"})\n",
    "def rag(question):\n",
    "    docs = retriever(question)\n",
    "    system_message = \"\"\"Answer the users question using only the provided information below:\n",
    "    \n",
    "    {docs}\"\"\".format(docs=\"\\n\".join(docs))\n",
    "    \n",
    "    return openai_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "        model=\"gpt-4o-mini\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-BT6TsepqEv38ZgtTuqD47w15b1Gsm', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Praveen Reddy is a passionate software engineer who thrives in fast-paced startups, dedicated to creating impactful technological solutions.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746276124, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_0392822090', usage=CompletionUsage(completion_tokens=25, prompt_tokens=90, total_tokens=115, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "run_id = str(uuid.uuid4())\n",
    "rag(\n",
    "    \"give me 1 liner about praveen\",\n",
    "    langsmith_extra={\"run_id\": run_id, \"metadata\": {\"user_id\": \"praveen\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from langsmith import traceable\n",
    "openai_client = wrap_openai(OpenAI())\n",
    "\n",
    "\n",
    "@traceable(run_type=\"retriever\")\n",
    "def retriever(query: str):\n",
    "    results = [f\"{content}\"]\n",
    "    return results\n",
    "\n",
    "@traceable(metadata={\"llm\": \"gpt-4o-mini\"},project_name=\"youtube-trail-1\")\n",
    "def rag(question):\n",
    "    docs = retriever(question)\n",
    "    system_message = \"\"\"Answer the users question using only the provided information below:\n",
    "    \n",
    "    {docs}\"\"\".format(docs=\"\\n\".join(docs))\n",
    "    \n",
    "    return openai_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "        model=\"gpt-4o-mini\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag(\n",
    "    \"give me 1 liner about praveen\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langsmith as ls\n",
    "from langsmith.wrappers import wrap_openai\n",
    "import openai\n",
    "\n",
    "client = wrap_openai(openai.Client())\n",
    "\n",
    "que = \"Hello, How are u ?\"\n",
    "\n",
    "def chat_pipeline(question: str) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "    return client.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "\n",
    "with ls.trace(\"Chat Pipeline\", \"chain\", project_name=\"youtube-trail-1\", inputs={\"question\": f\"{que}\"}) as rt:\n",
    "    answer = chat_pipeline(f\"{que}\")\n",
    "    rt.end(outputs={\"answer\": answer})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RunTree API bypasses the automatic @traceable and wrapper approaches, giving you explicit control over when and how runs start, end, and nest\n",
    "\n",
    "RunTree represents your overall pipeline, and you use create_child to add nested runs (e.g., individual LLM calls) \n",
    "LangSmith\n",
    "\n",
    "\n",
    "After posting each run with .post(), you should call .end()/.patch() (or .patchRun()) to finalize and update outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.run_trees import RunTree\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "\t{\"role\": \"user\", \"content\": \"Hi, WhatsUp?\"}\n",
    "]\n",
    "\n",
    "pipeline = RunTree(name=\"Chat Pipeline\", run_type=\"chain\", inputs={\"question\": messages}, project_name=\"youtube-trail-1\")\n",
    "pipeline.post()\n",
    "\n",
    "child = pipeline.create_child(name=\"OpenAI Call\", run_type=\"llm\", inputs={\"messages\": messages})\n",
    "child.post()    \n",
    "\n",
    "resp = openai.Client().chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "child.end(outputs={\"response\": resp.choices[0].message.content})\n",
    "child.patch()\n",
    "\n",
    "pipeline.end(outputs={\"answer\": resp.choices[0].message.content})\n",
    "pipeline.patch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "\n",
    "# (1) Prepare an input, e.g., a user question\n",
    "question = \"Can you summarize this morning's meetings?\"\n",
    "\n",
    "# (2) Create the top-level pipeline run\n",
    "pipeline = RunTree(\n",
    "    name=\"Chat Pipeline\",        # a human-readable label\n",
    "    run_type=\"chain\",            # type: chain, llm, tool, etc.\n",
    "    inputs={\"question\": question}\n",
    ")\n",
    "pipeline.post()                  # send the \"start\" event :contentReference[oaicite:5]{index=5}\n",
    "\n",
    "# (3) Optionally perform some retrieval or preprocessing\n",
    "context = \"During this morning's meeting, we solved all world conflict.\"\n",
    "messages = [\n",
    "    { \"role\": \"system\", \"content\": \"You are a helpful assistant. Please respond based on the context.\" },\n",
    "    { \"role\": \"user\",   \"content\": f\"Question: {question}\\nContext: {context}\" }\n",
    "]\n",
    "\n",
    "# (4) Create a child run for the LLM invocation\n",
    "child_llm_run = pipeline.create_child(\n",
    "    name=\"OpenAI Call\",\n",
    "    run_type=\"llm\",\n",
    "    inputs={\"messages\": messages},\n",
    ")\n",
    "child_llm_run.post()             # send child \"start\" event :contentReference[oaicite:6]{index=6}\n",
    "\n",
    "# (5) Execute the actual LLM call\n",
    "client = openai.Client()\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\", messages=messages\n",
    ")\n",
    "\n",
    "# (6) End the child run with outputs\n",
    "child_llm_run.end(outputs=chat_completion)  \n",
    "child_llm_run.patch()            # update the child run with end time & outputs :contentReference[oaicite:7]{index=7}\n",
    "\n",
    "# (7) End the pipeline run with final outputs\n",
    "pipeline.end(outputs={\"answer\": chat_completion.choices[0].message.content})\n",
    "pipeline.patch()                 # finalize and push updates :contentReference[oaicite:8]{index=8}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langsmith.run_trees import RunTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is increase in the percentage of quaterly results?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = RunTree(\n",
    "    name=\"QA Pipeline\",\n",
    "    run_type=\"chain\",\n",
    "    inputs={\"query\": query}\n",
    ")\n",
    "pipeline.post() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = pipeline.create_child(\n",
    "    name=\"Load & Split Documents\",\n",
    "    run_type=\"tool\",\n",
    "    inputs={\"path\": \"my_docs.txt\"}\n",
    ")\n",
    "loader.post()\n",
    "\n",
    "path=\"my_docs.txt\"\n",
    "\n",
    "def load_and_split(path: str):\n",
    "    loader = TextLoader(path)\n",
    "    docs = loader.load()                                                 \n",
    "    splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    return splitter.split_documents(docs)               \n",
    "docs = load_and_split(path)\n",
    "loader.end(outputs={\"num_chunks\": len(docs)})\n",
    "loader.patch()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ranker = pipeline.create_child(\n",
    "    name=\"Embed & Rank\",\n",
    "    run_type=\"tool\",\n",
    "    inputs={\"num_chunks\": len(docs)}\n",
    ")\n",
    "ranker.post()\n",
    "\n",
    "def embed_and_store(docs):\n",
    "    embeddings = OpenAIEmbeddings()  \n",
    "    store = Chroma.from_documents(docs, embeddings)     \n",
    "    retrieved = store.similarity_search(query, k=2)  \n",
    "    top_docs = sorted(retrieved, key=lambda d: len(d.page_content))\n",
    "    return top_docs\n",
    "\n",
    "top_docs = embed_and_store(docs)\n",
    "ranker.end(outputs={\"top_docs\": [d.metadata.get(\"source\") for d in top_docs]})\n",
    "ranker.patch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'my_docs.txt'}, page_content=\"ChinnoVinoSoft reported strong quarterly results, showcasing a 15% increase in revenue compared to the previous quarter. The company's net profit margin improved significantly, driven by robust sales in its cloud services division. Operational efficiency initiatives contributed to a 10% reduction in overall costs. Customer acquisition rates hit an all-time high, reflecting growing demand for ChinnoVinoSoft's innovative solutions. The management remains optimistic about sustained growth in the upcoming quarters.\")]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(top_docs, query: str):\n",
    "    context = \"\\n\\n\".join(d.page_content for d in top_docs)\n",
    "    return f\"Answer the question using only the context below:\\n\\n{context}\\n\\nQuestion: {query}\"\n",
    "\n",
    "prompt_run = pipeline.create_child(\n",
    "    name=\"Build Prompt\",\n",
    "    run_type=\"tool\",\n",
    "    inputs={\"top_docs\": [d.page_content for d in top_docs], \"query\": query}\n",
    ")\n",
    "prompt_run.post()\n",
    "\n",
    "prompt = build_prompt(top_docs, query)\n",
    "\n",
    "prompt_run.end(outputs={\"prompt\": prompt})\n",
    "prompt_run.patch() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\n\\n\".join(d.page_content for d in top_docs)\n",
    "prompt = f\"Answer using only this context:\\n\\n{context}\\n\\nQ: {query}\"\n",
    "\n",
    "llm_run = pipeline.create_child(\n",
    "    name=\"LLM Call\",\n",
    "    run_type=\"llm\",\n",
    "    inputs={\"prompt\": prompt}\n",
    ")\n",
    "llm_run.post()\n",
    "\n",
    "client = openai.Client()\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "\n",
    "answer = resp.choices[0].message.content\n",
    "llm_run.end(outputs={\"answer\": answer})\n",
    "llm_run.patch()\n",
    "\n",
    "pipeline.end(outputs={\"final_answer\": answer})\n",
    "pipeline.patch()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
